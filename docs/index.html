<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>decontamination API documentation</title>
<meta name="description" content="[![][License img]][License]
&lt;span style=&#34;margin-right: 0.5rem;&#34;&gt;&lt;/span&gt;
[![][MainRepo img]][MainRepo]
&lt;!--
&lt;span style=&#34;margin-right:
…" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>decontamination</code></h1>
</header>
<section id="section-intro">
<p><a href="http://www.cecill.info/licences/Licence_CeCILL-C_V1-en.txt"><img alt="" src="https://img.shields.io/badge/license-CeCILL--C-blue.svg"></a>
<span style="margin-right: 0.5rem;"></span>
<a href="https://gitlab.in2p3.fr/lpsc-euclid/decontamination"><img alt="" src="https://img.shields.io/badge/Main%20Repo-gitlab.in2p3.fr-success"></a></p>
<!--
<span style="margin-right: 0.5rem;"></span>
[![][AltRepo img]][AltRepo]
-->
<p><a href="http://lpsc.in2p3.fr/"
target="_blank"><img src="logo_lpsc.svg" alt="LPSC" height="72" /></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://www.ijclab.in2p3.fr/"
target="_blank"><img src="logo_ijclab.svg" alt="IJCLab" height="72" /></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="http://www.in2p3.fr/"
target="_blank"><img src="logo_in2p3.svg" alt="IN2P3" height="72" /></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="http://www.univ-grenoble-alpes.fr/" target="_blank" style="margin-right: 0rem;"><img src="logo_uga.svg" alt="UGA" height="72" /></a></p>
<p>A toolbox for performing systematics decontamination in cosmology analyses.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
########################################################################################################################

&#34;&#34;&#34;
.. include:: ../docs/header.md

A toolbox for performing systematics decontamination in cosmology analyses.
&#34;&#34;&#34;

########################################################################################################################

import numpy as np

########################################################################################################################
# JIT                                                                                                                  #
########################################################################################################################

from ._jit import CPU_OPTIMIZATION_AVAILABLE, GPU_OPTIMIZATION_AVAILABLE, jit

from ._jit import DeviceArray, device_array_from, device_array_empty, device_array_zeros, device_array_full

########################################################################################################################
# ALGO                                                                                                                 #
########################################################################################################################

from ._algo.som_abstract import SOM_Abstract

from ._algo.som_pca import SOM_PCA

from ._algo.som_batch import SOM_Batch

from ._algo.som_online import SOM_Online

########################################################################################################################
# UTILITIES                                                                                                            #
########################################################################################################################

def array_to_string(arr):

    s = np.array2string(arr, separator = &#39;, &#39;, suppress_small = True)

    return s.replace(&#39;[ &#39;, &#39;[&#39;).replace(&#39; ]&#39;, &#39;]&#39;)

########################################################################################################################
# EXPORTS                                                                                                              #
########################################################################################################################

__all__ = [
    &#39;CPU_OPTIMIZATION_AVAILABLE&#39;, &#39;GPU_OPTIMIZATION_AVAILABLE&#39;, &#39;jit&#39;,
    &#39;DeviceArray&#39;, &#39;device_array_from&#39;, &#39;device_array_empty&#39;, &#39;device_array_zeros&#39;, &#39;device_array_full&#39;,
    &#39;SOM_Abstract&#39;, &#39;SOM_PCA&#39;, &#39;SOM_Batch&#39;, &#39;SOM_Online&#39;,
]

########################################################################################################################</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="decontamination.device_array_from"><code class="name flex">
<span>def <span class="ident">device_array_from</span></span>(<span>array: np.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>New device array (see <code><a title="decontamination.DeviceArray" href="#decontamination.DeviceArray">DeviceArray</a></code>), initialized from a Numpy ndarray.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_array_from(array: np.ndarray):

    &#34;&#34;&#34;
    New device array (see `DeviceArray`), initialized from a Numpy ndarray.
    &#34;&#34;&#34;

    return DeviceArray(array.shape, array.dtype, content = array)</code></pre>
</details>
</dd>
<dt id="decontamination.device_array_empty"><code class="name flex">
<span>def <span class="ident">device_array_empty</span></span>(<span>shape: Union[Tuple[int], int], dtype: Type[np.float32] = np.float32)</span>
</code></dt>
<dd>
<div class="desc"><p>New device array (see <code><a title="decontamination.DeviceArray" href="#decontamination.DeviceArray">DeviceArray</a></code>), not initialized.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>Union[Tuple[int], int]</code></dt>
<dd>Desired shape for the new array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Desired data-type for the new array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_array_empty(shape: Union[Tuple[int], int], dtype: Type[np.single] = np.float32):

    &#34;&#34;&#34;
    New device array (see `DeviceArray`), not initialized.

    Parameters
    ----------
    shape : Union[Tuple[int], int]
        Desired shape for the new array.
    dtype : Type[np.single]
        Desired data-type for the new array.
    &#34;&#34;&#34;

    return DeviceArray(shape, dtype, content = None)</code></pre>
</details>
</dd>
<dt id="decontamination.device_array_zeros"><code class="name flex">
<span>def <span class="ident">device_array_zeros</span></span>(<span>shape: Union[Tuple[int], int], dtype: Type[np.float32] = np.float32)</span>
</code></dt>
<dd>
<div class="desc"><p>New device array (see <code><a title="decontamination.DeviceArray" href="#decontamination.DeviceArray">DeviceArray</a></code>), filled with <strong>0</strong>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>Union[Tuple[int], int]</code></dt>
<dd>Desired shape for the new array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Desired data-type for the new array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_array_zeros(shape: Union[Tuple[int], int], dtype: Type[np.single] = np.float32):

    &#34;&#34;&#34;
    New device array (see `DeviceArray`), filled with **0**.

    Parameters
    ----------
    shape : Union[Tuple[int], int]
        Desired shape for the new array.
    dtype : Type[np.single]
        Desired data-type for the new array.
    &#34;&#34;&#34;

    return DeviceArray(shape, dtype, content = 0)</code></pre>
</details>
</dd>
<dt id="decontamination.device_array_full"><code class="name flex">
<span>def <span class="ident">device_array_full</span></span>(<span>shape: Union[Tuple[int], int], value: Union[int, float], dtype: Type[np.float32] = np.float32)</span>
</code></dt>
<dd>
<div class="desc"><p>New device array (see <code><a title="decontamination.DeviceArray" href="#decontamination.DeviceArray">DeviceArray</a></code>), filled with <strong>value</strong>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>Union[Tuple[int], int]</code></dt>
<dd>Desired shape for the new array.</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>Union[int, float]</code></dt>
<dd>Desired value for the new array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Desired data-type for the new array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_array_full(shape: Union[Tuple[int], int], value: Union[int, float], dtype: Type[np.single] = np.float32):

    &#34;&#34;&#34;
    New device array (see `DeviceArray`), filled with **value**.

    Parameters
    ----------
    shape : Union[Tuple[int], int]
        Desired shape for the new array.
    value : Union[int, float]
        Desired value for the new array.
    dtype : Type[np.single]
        Desired data-type for the new array.
    &#34;&#34;&#34;

    return DeviceArray(shape, dtype, content = value)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="decontamination.jit"><code class="flex name class">
<span>class <span class="ident">jit</span></span>
<span>(</span><span>kernel: bool = False, parallel: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Decorator to recompile Python functions into native CPU/GPU ones.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>kernel</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicates whether this function is a CPU/GPU kernel (default: <strong>False</strong>).</dd>
<dt><strong><code>parallel</code></strong> :&ensp;<code>bool</code></dt>
<dd>Enables automatic parallelization when running on CPU (default: <strong>False</strong>).</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>@jit(parallel = False)
def foo_xpu(a, b):

    return a + b

@jit(kernel = True)
def foo_kernel(result, a, b):

    ########################################################################
    # !--BEGIN-CPU--

    for i in range(result.shape[0]):

        result[i] = foo_xpu(a[i], b[i])

    # !--END-CPU--
    ########################################################################
    # !--BEGIN-GPU--

    i = cu.grid(1)
    if i &lt; result.shape[0]:

        result[i] = foo_xpu(a[i], b[i])

    # !--END-GPU--
    ########################################################################

use_gpu = True
threads_per_block = 32

A = np.random.randn(100_000).astype(np.float32)
B = np.random.randn(100_000).astype(np.float32)

result = device_array_empty(100_000, dtype = np.float32)

foo_kernel[use_gpu, threads_per_block, result.shape[0]](result, A, B)

print(result.copy_to_host())
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class jit(object):

    &#34;&#34;&#34;
    Decorator to recompile Python functions into native CPU/GPU ones.
    &#34;&#34;&#34;

    ####################################################################################################################

    _METHOD_RE = re.compile(&#39;def[^(]+(\\(.*)&#39;, flags = re.DOTALL)

    _CPU_CODE_RE = re.compile(re.escape(&#39;!--BEGIN-CPU--&#39;) + &#39;.*?&#39; + re.escape(&#39;!--END-CPU--&#39;), re.DOTALL)

    _GPU_CODE_RE = re.compile(re.escape(&#39;!--BEGIN-GPU--&#39;) + &#39;.*?&#39; + re.escape(&#39;!--END-GPU--&#39;), re.DOTALL)

    ####################################################################################################################

    def __init__(self, kernel: bool = False, parallel: bool = False):

        &#34;&#34;&#34;
        Parameters
        ----------
        kernel : bool
            Indicates whether this function is a CPU/GPU kernel (default: **False**).
        parallel : bool
            Enables automatic parallelization when running on CPU (default: **False**).

        Example
        -------
            @jit(parallel = False)
            def foo_xpu(a, b):

                return a + b

            @jit(kernel = True)
            def foo_kernel(result, a, b):

                ########################################################################
                # !--BEGIN-CPU--

                for i in range(result.shape[0]):

                    result[i] = foo_xpu(a[i], b[i])

                # !--END-CPU--
                ########################################################################
                # !--BEGIN-GPU--

                i = cu.grid(1)
                if i &lt; result.shape[0]:

                    result[i] = foo_xpu(a[i], b[i])

                # !--END-GPU--
                ########################################################################

            use_gpu = True
            threads_per_block = 32

            A = np.random.randn(100_000).astype(np.float32)
            B = np.random.randn(100_000).astype(np.float32)

            result = device_array_empty(100_000, dtype = np.float32)

            foo_kernel[use_gpu, threads_per_block, result.shape[0]](result, A, B)

            print(result.copy_to_host())
        &#34;&#34;&#34;

        self.kernel = kernel
        self.parallel = parallel

    ####################################################################################################################

    _cnt = 0

    @classmethod
    def _get_unique_function_name(cls) -&gt; str:

        name = f&#39;__jit_f{cls._cnt}&#39;

        cls._cnt = cls._cnt + 1

        return name

    ####################################################################################################################

    @staticmethod
    def _patch_cpu_code(code: str) -&gt; str:

        return (
            jit._GPU_CODE_RE.sub(&#39;&#39;, code)
            .replace(&#39;_xpu&#39;, &#39;_cpu&#39;)
            .replace(&#39;xpu.local_empty&#39;, &#39;np.empty&#39;)
            .replace(&#39;xpu.shared_empty&#39;, &#39;np.empty&#39;)
            .replace(&#39;xpu.syncthreads&#39;, &#39;#######&#39;)
        )

    ####################################################################################################################

    @staticmethod
    def _patch_gpu_code(code: str) -&gt; str:

        return (
            jit._CPU_CODE_RE.sub(&#39;&#39;, code)
            .replace(&#39;_xpu&#39;, &#39;_gpu&#39;)
            .replace(&#39;xpu.local_empty&#39;, &#39;cu.local.array&#39;)
            .replace(&#39;xpu.shared_empty&#39;, &#39;cu.shared.array&#39;)
            .replace(&#39;xpu.syncthreads&#39;, &#39;cu.syncthreads&#39;)
        )

    ####################################################################################################################

    @staticmethod
    def _inject_cpu_funct(orig_funct: Callable, new_funct: Callable) -&gt; None:

        orig_funct.__globals__[orig_funct.__name__.replace(&#39;_xpu&#39;, &#39;_cpu&#39;)] = new_funct

    ####################################################################################################################

    @staticmethod
    def _inject_gpu_funct(orig_funct: Callable, new_funct: Callable) -&gt; None:

        orig_funct.__globals__[orig_funct.__name__.replace(&#39;_xpu&#39;, &#39;_gpu&#39;)] = new_funct

    ####################################################################################################################

    def __call__(self, funct: Callable):

        if not self.kernel and not funct.__name__.endswith(&#39;_xpu&#39;):

            raise Exception(f&#39;Function `{funct.__name__}` name must ends with `_xpu`&#39;)

        ################################################################################################################
        # SOURCE CODE                                                                                                  #
        ################################################################################################################

        code_raw = jit._METHOD_RE.search(inspect.getsource(funct)).group(1)

        ################################################################################################################
        # NUMBA ON GPU                                                                                                 #
        ################################################################################################################

        name_cpu = jit._get_unique_function_name()

        code_cpu = jit._patch_cpu_code(f&#39;def {name_cpu} {code_raw}&#39;)

        ################################################################################################################

        exec(code_cpu, funct.__globals__)

        funct_cpu = eval(name_cpu, funct.__globals__)

        if not self.kernel:

            jit._inject_cpu_funct(funct, nb.njit(funct_cpu, parallel = self.parallel) if CPU_OPTIMIZATION_AVAILABLE else funct_cpu)

        ################################################################################################################
        # NUMBA ON CPU                                                                                                 #
        ################################################################################################################

        name_gpu = jit._get_unique_function_name()

        code_gpu = jit._patch_gpu_code(f&#39;def {name_gpu} {code_raw}&#39;)

        ################################################################################################################

        exec(code_gpu, funct.__globals__)

        funct_gpu = eval(name_gpu, funct.__globals__)

        if not self.kernel:

            jit._inject_gpu_funct(funct, cu.jit(funct_gpu, device = True) if GPU_OPTIMIZATION_AVAILABLE else dont_call)

        ################################################################################################################
        # KERNEL                                                                                                       #
        ################################################################################################################

        funct = Kernel(funct_cpu, funct_gpu, parallel = self.parallel) if self.kernel else dont_call

        ################################################################################################################

        return funct</code></pre>
</details>
</dd>
<dt id="decontamination.DeviceArray"><code class="flex name class">
<span>class <span class="ident">DeviceArray</span></span>
<span>(</span><span>shape: Union[Tuple[int], int], dtype: Type[np.float32] = np.float32, content: Union[int, float, np.ndarray, ForwardRef(None)] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Device array to be used when calling a CPU/GPU kernel.</p>
<p>Prefer using primitives <code><a title="decontamination.device_array_from" href="#decontamination.device_array_from">device_array_from()</a></code>, <code><a title="decontamination.device_array_empty" href="#decontamination.device_array_empty">device_array_empty()</a></code>, <code><a title="decontamination.device_array_zeros" href="#decontamination.device_array_zeros">device_array_zeros()</a></code>, <code><a title="decontamination.device_array_full" href="#decontamination.device_array_full">device_array_full()</a></code>
to instantiate a device array.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>Union[Tuple[int], int]</code></dt>
<dd>Desired shape for the new array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Desired data-type for the new array.</dd>
<dt><strong><code>content</code></strong> :&ensp;<code>Optional[Union[int, float, np.ndarray]]</code></dt>
<dd>Optional content, integer, floating ot Numpy ndarray.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeviceArray(object):

    &#34;&#34;&#34;
    Device array to be used when calling a CPU/GPU kernel.

    Prefer using primitives `device_array_from`, `device_array_empty`, `device_array_zeros`, `device_array_full`
    to instantiate a device array.
    &#34;&#34;&#34;

    ####################################################################################################################

    def __init__(self, shape: Union[Tuple[int], int], dtype: Type[np.single] = np.float32, content: Optional[Union[int, float, np.ndarray]] = None):

        &#34;&#34;&#34;
        Parameters
        ----------
        shape : Union[Tuple[int], int]
            Desired shape for the new array.
        dtype : Type[np.single]
            Desired data-type for the new array.
        content : Optional[Union[int, float, np.ndarray]]
            Optional content, integer, floating ot Numpy ndarray.
        &#34;&#34;&#34;

        self._shape = shape
        self._dtype = dtype

        self._content = content

        self._instance = None

    ####################################################################################################################

    @property
    def shape(self):

        &#34;&#34;&#34;
        Shape of the array.
        &#34;&#34;&#34;

        return self._shape

    ####################################################################################################################

    @property
    def dtype(self):

        &#34;&#34;&#34;
        Data-type of the array.
        &#34;&#34;&#34;

        return self._dtype

    ####################################################################################################################

    def _instantiate(self, is_gpu: bool):

        ################################################################################################################

        if self._instance is not None:

            raise Exception(&#39;Device array already instanced&#39;)

        ################################################################################################################

        if is_gpu:

            ############################################################################################################
            # GPU INSTANTIATION                                                                                        #
            ############################################################################################################

            if isinstance(self._content, np.ndarray):

                self._instance = cu.to_device(self._content)

            elif self._content is not None:

                if float(self._content) == 0.0:
                    self._instance = cu.device_array(shape = self._shape, dtype = self._dtype)
                else:
                    self._instance = cu.device_array_like(np.full(shape = self._shape, fill_value = self._content, dtype = self._dtype))

            else:

                self._instance = cu.device_array(shape = self._shape, dtype = self._dtype)

            ############################################################################################################

        else:

            ############################################################################################################
            # CPU INSTANTIATION                                                                                        #
            ############################################################################################################

            if isinstance(self._content, np.ndarray):

                self._instance = nb_to_device(self._content)

            elif self._content is not None:

                if float(self._content) == 0.0:
                    self._instance = np.zeros(shape = self._shape, dtype = self._dtype)
                else:
                    self._instance = np.full(shape = self._shape, fill_value = self._content, dtype = self._dtype)

            else:

                self._instance = np.empty(shape = self._shape, dtype = self._dtype)

        ################################################################################################################

        return self._instance

    ####################################################################################################################

    def copy_to_host(self) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Create a new Numpy ndarray from the underlying device ndarray.
        &#34;&#34;&#34;

        ################################################################################################################

        if self._instance is None:

            raise Exception(&#39;Device array not instanced&#39;)

        ################################################################################################################

        if cu.is_cuda_array(self._instance):

            return self._instance.copy_to_host()

        else:

            return self._instance</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="decontamination.DeviceArray.shape"><code class="name">var <span class="ident">shape</span></code></dt>
<dd>
<div class="desc"><p>Shape of the array.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self):

    &#34;&#34;&#34;
    Shape of the array.
    &#34;&#34;&#34;

    return self._shape</code></pre>
</details>
</dd>
<dt id="decontamination.DeviceArray.dtype"><code class="name">var <span class="ident">dtype</span></code></dt>
<dd>
<div class="desc"><p>Data-type of the array.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dtype(self):

    &#34;&#34;&#34;
    Data-type of the array.
    &#34;&#34;&#34;

    return self._dtype</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="decontamination.DeviceArray.copy_to_host"><code class="name flex">
<span>def <span class="ident">copy_to_host</span></span>(<span>self) ‑> np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Create a new Numpy ndarray from the underlying device ndarray.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy_to_host(self) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Create a new Numpy ndarray from the underlying device ndarray.
    &#34;&#34;&#34;

    ################################################################################################################

    if self._instance is None:

        raise Exception(&#39;Device array not instanced&#39;)

    ################################################################################################################

    if cu.is_cuda_array(self._instance):

        return self._instance.copy_to_host()

    else:

        return self._instance</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="decontamination.SOM_Abstract"><code class="flex name class">
<span>class <span class="ident">SOM_Abstract</span></span>
<span>(</span><span>m: int, n: int, dim: int, dtype: Type[np.float32] = np.float32, topology: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Self Organizing Maps (abstract class).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron rows.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron columns.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of the input data.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Neural network data type (default: <strong>np.float32</strong>).</dd>
<dt><strong><code>topology</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Topology of the map, either '<strong>square</strong>' or '<strong>hexagonal</strong>' (default: '<strong>hexagonal</strong>').</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SOM_Abstract(object):

    &#34;&#34;&#34;
    Self Organizing Maps (abstract class).
    &#34;&#34;&#34;

    __MODE__ = &#39;abstract&#39;

    ####################################################################################################################

    def __init__(self, m: int, n: int, dim: int, dtype: Type[np.single] = np.float32, topology: Optional[str] = None):

        &#34;&#34;&#34;
        Parameters
        ----------
        m : int
            Number of neuron rows.
        n : int
            Number of neuron columns.
        dim : int
            Dimensionality of the input data.
        dtype : Type[np.single]
            Neural network data type (default: **np.float32**).
        topology : Optional[str]
            Topology of the map, either &#39;**square**&#39; or &#39;**hexagonal**&#39; (default: &#39;**hexagonal**&#39;).
        &#34;&#34;&#34;

        ################################################################################################################

        self._m = m
        self._n = n
        self._dim = dim
        self._dtype = dtype
        self._topology = topology or &#39;hexagonal&#39;

        ################################################################################################################

        self._rebuild_topography()

        ################################################################################################################

        self._weights = np.empty(shape = (self._m * self._n, self._dim), dtype = self._dtype)

        self._quantization_errors = np.empty(0, dtype = np.float32)

        self._topographic_errors = np.empty(0, dtype = np.float32)

        ################################################################################################################

        self._header_extra = {
            &#39;mode&#39;: &#39;__MODE__&#39;,
        }

        self._dataset_extra = {
        }

    ####################################################################################################################

    @staticmethod
    def _neuron_locations_square(m: int, n: int) -&gt; Iterator[List[int]]:

        for i in range(m):
            for j in range(n):

                yield [i, j]

    ####################################################################################################################

    @staticmethod
    def _neuron_locations_hexagonal(m: int, n: int) -&gt; Iterator[List[float]]:

        for i in range(m):
            for j in range(n):

                i_offset = (j &amp; 1) * (-0.5)

                yield [i + i_offset, j * 0.8660254037844386]  # √3/2 = 0.8660254037844386

    ####################################################################################################################
    def _rebuild_topography(self):

        if self._topology == &#39;square&#39;:

            self._topography = np.array(list(SOM_Abstract._neuron_locations_square(self._m, self._n)), dtype = np.float32)

        else:

            self._topography = np.array(list(SOM_Abstract._neuron_locations_hexagonal(self._m, self._n)), dtype = np.float32)

    ####################################################################################################################

    def init_rand(self, seed: Optional[int] = None) -&gt; None:

        &#34;&#34;&#34;
        Initializes the neural network randomly.

        Parameters
        ----------
        seed : Optional[int]
            Seed for random generator (default: **None**).
        &#34;&#34;&#34;

        ################################################################################################################

        if seed is None:

            rng = np.random.default_rng()

        else:

            rng = np.random.default_rng(seed = seed)

        ################################################################################################################

        self._weights[...] = rng.random((self._m * self._n, self._dim), dtype = self._dtype)

    ####################################################################################################################

    def init_from(self, other: &#39;SOM_Abstract&#39;) -&gt; None:

        &#34;&#34;&#34;
        Initializes the neural network from another one.

        Parameters
        ----------
        other : SOM_Abstract
            Another SOM object from which the weights will be copied.
        &#34;&#34;&#34;

        if self._m != other._m              \
           or                               \
           self._n != other._n              \
           or                               \
           self._dim != other._dim          \
           or                               \
           self._dtype != other._dtype      \
           or                               \
           self._topology != other._topology:

            raise Exception(&#39;Incompatible shapes, dtypes or topologies&#39;)

        self._weights[...] = other._weights

    ####################################################################################################################

    def _init_hdf5_extra(self):

        ################################################################################################################

        header_extra = {name: field for name, field in self._header_extra.items()}

        dataset_extra = {name: field for name, field in self._dataset_extra.items()}

        ################################################################################################################

        header_extra[&#39;m&#39;] = &#39;_m&#39;
        header_extra[&#39;n&#39;] = &#39;_n&#39;
        header_extra[&#39;dim&#39;] = &#39;_dim&#39;
        header_extra[&#39;topology&#39;] = &#39;_topology&#39;

        dataset_extra[&#39;weights&#39;] = &#39;_weights&#39;
        dataset_extra[&#39;quantization_errors&#39;] = &#39;_quantization_errors&#39;
        dataset_extra[&#39;topographic_errors&#39;] = &#39;_topographic_errors&#39;

        ################################################################################################################

        return header_extra, dataset_extra

    ####################################################################################################################

    def save(self, filename: str) -&gt; None:

        &#34;&#34;&#34;
        Saves the trained neural network to a HDF5 file.

        Parameters
        ----------
        filename : str
            Output filename.
        &#34;&#34;&#34;

        ################################################################################################################

        import h5py

        ################################################################################################################

        header_extra, dataset_extra = self._init_hdf5_extra()

        ################################################################################################################

        with h5py.File(filename, mode = &#39;w&#39;) as file:

            model_group = file.create_group(&#39;model&#39;, track_order = True)

            # HEADERS #

            for name, field in header_extra.items():

                data = getattr(self, field)

                if data is not None:

                    model_group.attrs[name] = data

            # DATASETS #

            for name, field in dataset_extra.items():

                data = getattr(self, field)

                if data is not None:

                    model_group.create_dataset(
                        name,
                        data = data,
                        shape = data.shape,
                        dtype = data.dtype
                    )

        ################################################################################################################

        self._rebuild_topography()

    ####################################################################################################################

    def load(self, filename: str) -&gt; None:

        &#34;&#34;&#34;
        Loads the trained neural network from a HDF5 file.

        Parameters
        ----------
        filename : str
            Input filename.
        &#34;&#34;&#34;

        ################################################################################################################

        import h5py

        ################################################################################################################

        header_extra, dataset_extra = self._init_hdf5_extra()

        ################################################################################################################

        with h5py.File(filename, mode = &#39;r&#39;) as file:

            model_group = file[&#39;model&#39;]

            # HEADERS #

            for name, field in header_extra.items():

                try:
                    setattr(self, field, np.array(model_group.attrs[name]))
                except KeyError:
                    pass

            # DATASETS #

            for name, field in dataset_extra.items():

                try:
                    setattr(self, field, np.array(model_group[name]))
                except KeyError:
                    pass

        ################################################################################################################

        self._rebuild_topography()

    ####################################################################################################################

    def get_weights(self) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns the neural network weights with the shape [m * n, dim].
        &#34;&#34;&#34;

        return self._weights.reshape((self._m * self._n, self._dim))

    ####################################################################################################################

    def get_centroids(self) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns the neural network weights with the shape [m, n, dim].
        &#34;&#34;&#34;

        return self._weights.reshape((self._m, self._n, self._dim))

    ####################################################################################################################

    def get_quantization_errors(self) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns the quantization error. $$ c_i^1\\equiv\\mathrm{1^\\mathrm{st}\\,bmu}\\equiv\\underset{j}{\\mathrm{arg\\,min}_1}\\lVert x_i-w_j\\rVert $$ $$ \\boxed{e_Q\\equiv\\frac{1}{N}\\sum_{i=1}^N\\lVert x_i-w_{c_i^1}\\rVert} $$
        &#34;&#34;&#34;

        return self._quantization_errors

    ####################################################################################################################

    def get_topographic_errors(self) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns the topographic errors. $$ c_i^n\\equiv\\mathrm{n^\\mathrm{th}\\,bmu}\\equiv\\underset{j}{\\mathrm{arg\\,min}_n}\\lVert x_i-w_j\\rVert $$ $$ r\\equiv\\left\\{\\begin{array}{ll}\\sqrt{1}&amp;\\mathrm{topology=hexagon}\\\\\\sqrt{2}&amp;\\mathrm{topology=square}\\end{array}\\right. $$ $$ t(x_i)\\equiv\\left\\{\\begin{array}{ll}1&amp;\\lVert c_i^1-c_i^2\\rVert&gt;r\\\\0&amp;\\mathrm{otherwise}\\end{array}\\right. $$ $$ \\boxed{e_t\\equiv\\frac{1}{N}\\sum_{i=0}^Nt(x_i)} $$
        &#34;&#34;&#34;

        return self._topographic_errors

    ####################################################################################################################

    _X_HEX_STENCIL = np.array([
        +1, +1, +1, +0, -1, +0,  # Even line
        +0, +1, +0, -1, -1, -1,  # Odd line
    ], dtype = np.int64)

    _Y_HEX_STENCIL = np.array([
        +1, +0, -1, -1, +0, +1,  # Even line
        +1, +0, -1, -1, +0, +1,  # Odd line
    ], dtype = np.int64)

    ####################################################################################################################

    _X_SQU_STENCIL = np.array([
        +0, -1, -1, -1, +0, +1, +1, +1,  # Even line
        +0, -1, -1, -1, +0, +1, +1, +1,  # Odd line
    ], dtype = np.int64)

    _Y_SQU_STENCIL = np.array([
        -1, -1, +0, +1, +1, +1, +0, -1,  # Even line
        -1, -1, +0, +1, +1, +1, +0, -1,  # Odd line
    ], dtype = np.int64)

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _distance_map(result, centroids: np.ndarray, x_stencil: np.ndarray, y_stencil: np.ndarray, m: int, n: int, l: int) -&gt; None:

        for x in range(m):
            for y in range(n):

                offset = (y &amp; 1) * l

                w1 = centroids[x, y]

                for z in range(l):

                    i = x + x_stencil[z + offset]
                    j = y + y_stencil[z + offset]

                    if 0 &lt;= i &lt; m and 0 &lt;= j &lt; n:

                        w2 = centroids[i, j]

                        result[x, y, z] = np.sqrt(np.sum((w1 - w2) ** 2))

    ####################################################################################################################

    def get_distance_map(self, scaling: Optional[str] = None) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns a matrix of the distances between each weight.

        Parameters
        ----------
        scaling : Optional[str]
            Normalization method, either &#39;**sum**&#39; or &#39;**mean**&#39; (default: &#39;**sum**&#39;).
        &#34;&#34;&#34;

        scaling = scaling or &#39;sum&#39;

        ################################################################################################################

        if self._topology == &#39;square&#39;:

            result = np.full(shape = (self._m, self._n, 8), fill_value = np.nan, dtype = self._dtype)

            SOM_Abstract._distance_map(result, self.get_centroids(), SOM_Abstract._X_SQU_STENCIL, SOM_Abstract._Y_SQU_STENCIL, self._m, self._n, 8)

        else:

            result = np.full(shape = (self._m, self._n, 6), fill_value = np.nan, dtype = self._dtype)

            SOM_Abstract._distance_map(result, self.get_centroids(), SOM_Abstract._X_HEX_STENCIL, SOM_Abstract._Y_HEX_STENCIL, self._m, self._n, 6)

        ################################################################################################################

        if scaling == &#39;sum&#39;:
            result = np.nansum(result, axis = 2)

        elif scaling == &#39;mean&#39;:
            result = np.nanmean(result, axis = 2)

        else:
            raise Exception(f&#39;Invalid scaling method `{scaling}`&#39;)

        ################################################################################################################

        return result / np.max(result)

    ####################################################################################################################

    def get_activation_map(self, dataset: Union[np.ndarray, Callable], enable_gpu: bool = True, threads_per_blocks: Union[Tuple[int], int] = 1024) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns a matrix containing the number of times each neuron have been activated for the given input.

        Parameters
        ----------
        dataset : Union[np.ndarray, Callable]
            Dataset array or generator builder.
        enable_gpu : bool
            If available, run on GPU rather than CPU (default: **True**)
        threads_per_blocks : Union[Tuple[int], int]
            Number of GPU threads per blocks (default: **1024**)
        &#34;&#34;&#34;

        ################################################################################################################

        generator_builder = dataset_to_generator_builder(dataset)

        generator = generator_builder()

        ################################################################################################################

        result = device_array_zeros(self._m * self._n, dtype = np.int64)

        ################################################################################################################

        for data in generator():

            _count_bmus_kernel[enable_gpu, threads_per_blocks, data.shape[0]](result, self._weights, data, self._m * self._n)

        ################################################################################################################

        return result.copy_to_host().reshape((self._m, self._n, ))

    ####################################################################################################################

    def get_winners(self, dataset: np.ndarray, enable_gpu: bool = True, threads_per_blocks: Union[Tuple[int], int] = 1024) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns a vector of the best matching unit indices (shape m×n) for the given input.

        Parameters
        ----------
        dataset : np.ndarray
            Dataset array.
        enable_gpu : bool
            If available, run on GPU rather than CPU (default: **True**)
        threads_per_blocks : Union[Tuple[int], int]
            Number of GPU threads per blocks (default: **1024**)
        &#34;&#34;&#34;

        ################################################################################################################

        result = device_array_empty(dataset.shape[0], dtype = np.int32)

        ################################################################################################################

        _find_bmus_kernel[enable_gpu, threads_per_blocks, dataset.shape[0]](result, self._weights, dataset, self._m * self._n)

        ################################################################################################################

        return result.copy_to_host()</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li>decontamination._algo.som_batch.SOM_Batch</li>
<li>decontamination._algo.som_online.SOM_Online</li>
<li>decontamination._algo.som_pca.SOM_PCA</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="decontamination.SOM_Abstract.init_rand"><code class="name flex">
<span>def <span class="ident">init_rand</span></span>(<span>self, seed: Optional[int] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes the neural network randomly.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>seed</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Seed for random generator (default: <strong>None</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_rand(self, seed: Optional[int] = None) -&gt; None:

    &#34;&#34;&#34;
    Initializes the neural network randomly.

    Parameters
    ----------
    seed : Optional[int]
        Seed for random generator (default: **None**).
    &#34;&#34;&#34;

    ################################################################################################################

    if seed is None:

        rng = np.random.default_rng()

    else:

        rng = np.random.default_rng(seed = seed)

    ################################################################################################################

    self._weights[...] = rng.random((self._m * self._n, self._dim), dtype = self._dtype)</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.init_from"><code class="name flex">
<span>def <span class="ident">init_from</span></span>(<span>self, other: <a title="decontamination.SOM_Abstract" href="#decontamination.SOM_Abstract">SOM_Abstract</a>) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes the neural network from another one.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>other</code></strong> :&ensp;<code><a title="decontamination.SOM_Abstract" href="#decontamination.SOM_Abstract">SOM_Abstract</a></code></dt>
<dd>Another SOM object from which the weights will be copied.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_from(self, other: &#39;SOM_Abstract&#39;) -&gt; None:

    &#34;&#34;&#34;
    Initializes the neural network from another one.

    Parameters
    ----------
    other : SOM_Abstract
        Another SOM object from which the weights will be copied.
    &#34;&#34;&#34;

    if self._m != other._m              \
       or                               \
       self._n != other._n              \
       or                               \
       self._dim != other._dim          \
       or                               \
       self._dtype != other._dtype      \
       or                               \
       self._topology != other._topology:

        raise Exception(&#39;Incompatible shapes, dtypes or topologies&#39;)

    self._weights[...] = other._weights</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, filename: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the trained neural network to a HDF5 file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Output filename.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, filename: str) -&gt; None:

    &#34;&#34;&#34;
    Saves the trained neural network to a HDF5 file.

    Parameters
    ----------
    filename : str
        Output filename.
    &#34;&#34;&#34;

    ################################################################################################################

    import h5py

    ################################################################################################################

    header_extra, dataset_extra = self._init_hdf5_extra()

    ################################################################################################################

    with h5py.File(filename, mode = &#39;w&#39;) as file:

        model_group = file.create_group(&#39;model&#39;, track_order = True)

        # HEADERS #

        for name, field in header_extra.items():

            data = getattr(self, field)

            if data is not None:

                model_group.attrs[name] = data

        # DATASETS #

        for name, field in dataset_extra.items():

            data = getattr(self, field)

            if data is not None:

                model_group.create_dataset(
                    name,
                    data = data,
                    shape = data.shape,
                    dtype = data.dtype
                )

    ################################################################################################################

    self._rebuild_topography()</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, filename: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the trained neural network from a HDF5 file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Input filename.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, filename: str) -&gt; None:

    &#34;&#34;&#34;
    Loads the trained neural network from a HDF5 file.

    Parameters
    ----------
    filename : str
        Input filename.
    &#34;&#34;&#34;

    ################################################################################################################

    import h5py

    ################################################################################################################

    header_extra, dataset_extra = self._init_hdf5_extra()

    ################################################################################################################

    with h5py.File(filename, mode = &#39;r&#39;) as file:

        model_group = file[&#39;model&#39;]

        # HEADERS #

        for name, field in header_extra.items():

            try:
                setattr(self, field, np.array(model_group.attrs[name]))
            except KeyError:
                pass

        # DATASETS #

        for name, field in dataset_extra.items():

            try:
                setattr(self, field, np.array(model_group[name]))
            except KeyError:
                pass

    ################################################################################################################

    self._rebuild_topography()</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_weights"><code class="name flex">
<span>def <span class="ident">get_weights</span></span>(<span>self) ‑> np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the neural network weights with the shape [m * n, dim].</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_weights(self) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns the neural network weights with the shape [m * n, dim].
    &#34;&#34;&#34;

    return self._weights.reshape((self._m * self._n, self._dim))</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_centroids"><code class="name flex">
<span>def <span class="ident">get_centroids</span></span>(<span>self) ‑> np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the neural network weights with the shape [m, n, dim].</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_centroids(self) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns the neural network weights with the shape [m, n, dim].
    &#34;&#34;&#34;

    return self._weights.reshape((self._m, self._n, self._dim))</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_quantization_errors"><code class="name flex">
<span>def <span class="ident">get_quantization_errors</span></span>(<span>self) ‑> np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the quantization error. <span><span class="MathJax_Preview"> c_i^1\equiv\mathrm{1^\mathrm{st}\,bmu}\equiv\underset{j}{\mathrm{arg\,min}_1}\lVert x_i-w_j\rVert </span><script type="math/tex; mode=display"> c_i^1\equiv\mathrm{1^\mathrm{st}\,bmu}\equiv\underset{j}{\mathrm{arg\,min}_1}\lVert x_i-w_j\rVert </script></span> <span><span class="MathJax_Preview"> \boxed{e_Q\equiv\frac{1}{N}\sum_{i=1}^N\lVert x_i-w_{c_i^1}\rVert} </span><script type="math/tex; mode=display"> \boxed{e_Q\equiv\frac{1}{N}\sum_{i=1}^N\lVert x_i-w_{c_i^1}\rVert} </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_quantization_errors(self) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns the quantization error. $$ c_i^1\\equiv\\mathrm{1^\\mathrm{st}\\,bmu}\\equiv\\underset{j}{\\mathrm{arg\\,min}_1}\\lVert x_i-w_j\\rVert $$ $$ \\boxed{e_Q\\equiv\\frac{1}{N}\\sum_{i=1}^N\\lVert x_i-w_{c_i^1}\\rVert} $$
    &#34;&#34;&#34;

    return self._quantization_errors</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_topographic_errors"><code class="name flex">
<span>def <span class="ident">get_topographic_errors</span></span>(<span>self) ‑> np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the topographic errors. <span><span class="MathJax_Preview"> c_i^n\equiv\mathrm{n^\mathrm{th}\,bmu}\equiv\underset{j}{\mathrm{arg\,min}_n}\lVert x_i-w_j\rVert </span><script type="math/tex; mode=display"> c_i^n\equiv\mathrm{n^\mathrm{th}\,bmu}\equiv\underset{j}{\mathrm{arg\,min}_n}\lVert x_i-w_j\rVert </script></span> <span><span class="MathJax_Preview"> r\equiv\left\{\begin{array}{ll}\sqrt{1}&amp;\mathrm{topology=hexagon}\\\sqrt{2}&amp;\mathrm{topology=square}\end{array}\right. </span><script type="math/tex; mode=display"> r\equiv\left\{\begin{array}{ll}\sqrt{1}&\mathrm{topology=hexagon}\\\sqrt{2}&\mathrm{topology=square}\end{array}\right. </script></span> <span><span class="MathJax_Preview"> t(x_i)\equiv\left\{\begin{array}{ll}1&amp;\lVert c_i^1-c_i^2\rVert&gt;r\\0&amp;\mathrm{otherwise}\end{array}\right. </span><script type="math/tex; mode=display"> t(x_i)\equiv\left\{\begin{array}{ll}1&\lVert c_i^1-c_i^2\rVert>r\\0&\mathrm{otherwise}\end{array}\right. </script></span> <span><span class="MathJax_Preview"> \boxed{e_t\equiv\frac{1}{N}\sum_{i=0}^Nt(x_i)} </span><script type="math/tex; mode=display"> \boxed{e_t\equiv\frac{1}{N}\sum_{i=0}^Nt(x_i)} </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_topographic_errors(self) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns the topographic errors. $$ c_i^n\\equiv\\mathrm{n^\\mathrm{th}\\,bmu}\\equiv\\underset{j}{\\mathrm{arg\\,min}_n}\\lVert x_i-w_j\\rVert $$ $$ r\\equiv\\left\\{\\begin{array}{ll}\\sqrt{1}&amp;\\mathrm{topology=hexagon}\\\\\\sqrt{2}&amp;\\mathrm{topology=square}\\end{array}\\right. $$ $$ t(x_i)\\equiv\\left\\{\\begin{array}{ll}1&amp;\\lVert c_i^1-c_i^2\\rVert&gt;r\\\\0&amp;\\mathrm{otherwise}\\end{array}\\right. $$ $$ \\boxed{e_t\\equiv\\frac{1}{N}\\sum_{i=0}^Nt(x_i)} $$
    &#34;&#34;&#34;

    return self._topographic_errors</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_distance_map"><code class="name flex">
<span>def <span class="ident">get_distance_map</span></span>(<span>self, scaling: Optional[str] = None) ‑> np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a matrix of the distances between each weight.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>scaling</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Normalization method, either '<strong>sum</strong>' or '<strong>mean</strong>' (default: '<strong>sum</strong>').</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_distance_map(self, scaling: Optional[str] = None) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns a matrix of the distances between each weight.

    Parameters
    ----------
    scaling : Optional[str]
        Normalization method, either &#39;**sum**&#39; or &#39;**mean**&#39; (default: &#39;**sum**&#39;).
    &#34;&#34;&#34;

    scaling = scaling or &#39;sum&#39;

    ################################################################################################################

    if self._topology == &#39;square&#39;:

        result = np.full(shape = (self._m, self._n, 8), fill_value = np.nan, dtype = self._dtype)

        SOM_Abstract._distance_map(result, self.get_centroids(), SOM_Abstract._X_SQU_STENCIL, SOM_Abstract._Y_SQU_STENCIL, self._m, self._n, 8)

    else:

        result = np.full(shape = (self._m, self._n, 6), fill_value = np.nan, dtype = self._dtype)

        SOM_Abstract._distance_map(result, self.get_centroids(), SOM_Abstract._X_HEX_STENCIL, SOM_Abstract._Y_HEX_STENCIL, self._m, self._n, 6)

    ################################################################################################################

    if scaling == &#39;sum&#39;:
        result = np.nansum(result, axis = 2)

    elif scaling == &#39;mean&#39;:
        result = np.nanmean(result, axis = 2)

    else:
        raise Exception(f&#39;Invalid scaling method `{scaling}`&#39;)

    ################################################################################################################

    return result / np.max(result)</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_activation_map"><code class="name flex">
<span>def <span class="ident">get_activation_map</span></span>(<span>self, dataset: Union[np.ndarray, Callable], enable_gpu: bool = True, threads_per_blocks: Union[Tuple[int], int] = 1024) ‑> np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a matrix containing the number of times each neuron have been activated for the given input.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Union[np.ndarray, Callable]</code></dt>
<dd>Dataset array or generator builder.</dd>
<dt><strong><code>enable_gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>If available, run on GPU rather than CPU (default: <strong>True</strong>)</dd>
<dt><strong><code>threads_per_blocks</code></strong> :&ensp;<code>Union[Tuple[int], int]</code></dt>
<dd>Number of GPU threads per blocks (default: <strong>1024</strong>)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_activation_map(self, dataset: Union[np.ndarray, Callable], enable_gpu: bool = True, threads_per_blocks: Union[Tuple[int], int] = 1024) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns a matrix containing the number of times each neuron have been activated for the given input.

    Parameters
    ----------
    dataset : Union[np.ndarray, Callable]
        Dataset array or generator builder.
    enable_gpu : bool
        If available, run on GPU rather than CPU (default: **True**)
    threads_per_blocks : Union[Tuple[int], int]
        Number of GPU threads per blocks (default: **1024**)
    &#34;&#34;&#34;

    ################################################################################################################

    generator_builder = dataset_to_generator_builder(dataset)

    generator = generator_builder()

    ################################################################################################################

    result = device_array_zeros(self._m * self._n, dtype = np.int64)

    ################################################################################################################

    for data in generator():

        _count_bmus_kernel[enable_gpu, threads_per_blocks, data.shape[0]](result, self._weights, data, self._m * self._n)

    ################################################################################################################

    return result.copy_to_host().reshape((self._m, self._n, ))</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_winners"><code class="name flex">
<span>def <span class="ident">get_winners</span></span>(<span>self, dataset: np.ndarray, enable_gpu: bool = True, threads_per_blocks: Union[Tuple[int], int] = 1024) ‑> np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a vector of the best matching unit indices (shape m×n) for the given input.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Dataset array.</dd>
<dt><strong><code>enable_gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>If available, run on GPU rather than CPU (default: <strong>True</strong>)</dd>
<dt><strong><code>threads_per_blocks</code></strong> :&ensp;<code>Union[Tuple[int], int]</code></dt>
<dd>Number of GPU threads per blocks (default: <strong>1024</strong>)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_winners(self, dataset: np.ndarray, enable_gpu: bool = True, threads_per_blocks: Union[Tuple[int], int] = 1024) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns a vector of the best matching unit indices (shape m×n) for the given input.

    Parameters
    ----------
    dataset : np.ndarray
        Dataset array.
    enable_gpu : bool
        If available, run on GPU rather than CPU (default: **True**)
    threads_per_blocks : Union[Tuple[int], int]
        Number of GPU threads per blocks (default: **1024**)
    &#34;&#34;&#34;

    ################################################################################################################

    result = device_array_empty(dataset.shape[0], dtype = np.int32)

    ################################################################################################################

    _find_bmus_kernel[enable_gpu, threads_per_blocks, dataset.shape[0]](result, self._weights, dataset, self._m * self._n)

    ################################################################################################################

    return result.copy_to_host()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="decontamination.SOM_PCA"><code class="flex name class">
<span>class <span class="ident">SOM_PCA</span></span>
<span>(</span><span>m: int, n: int, dim: int, dtype: Type[np.float32] = np.float32, topology: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Self Organizing Maps that span the first two principal components.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron rows.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron columns.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of the input data.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Neural network data type (default: <strong>np.float32</strong>).</dd>
<dt><strong><code>topology</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Topology of the map, either '<strong>square</strong>' or '<strong>hexagonal</strong>' (default: '<strong>hexagonal</strong>').</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SOM_PCA(som_abstract.SOM_Abstract):

    &#34;&#34;&#34;
    Self Organizing Maps that span the first two principal components.
    &#34;&#34;&#34;

    __MODE__ = &#39;pca&#39;

    ####################################################################################################################

    def __init__(self, m: int, n: int, dim: int, dtype: Type[np.single] = np.float32, topology: Optional[str] = None):

        &#34;&#34;&#34;
        Parameters
        ----------
        m : int
            Number of neuron rows.
        n : int
            Number of neuron columns.
        dim : int
            Dimensionality of the input data.
        dtype : Type[np.single]
            Neural network data type (default: **np.float32**).
        topology : Optional[str]
            Topology of the map, either &#39;**square**&#39; or &#39;**hexagonal**&#39; (default: &#39;**hexagonal**&#39;).
        &#34;&#34;&#34;

        ################################################################################################################

        super().__init__(m, n, dim, dtype, topology)

        ################################################################################################################

        self._header_extra = {
            &#39;mode&#39;: &#39;__MODE__&#39;,
        }

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _update_cov_matrix(result_sum: np.ndarray, result_prods: np.ndarray, data: np.ndarray) -&gt; None:

        ################################################################################################################

        data_dim = data.shape[0]
        syst_dim = data.shape[1]

        ################################################################################################################

        for i in range(data_dim):

            value = data[i].astype(np.float64)

            for j in range(syst_dim):

                value_j = value[j]
                result_sum[j] += value_j

                for k in range(syst_dim):

                    value_jk = value_j * value[k]
                    result_prods[j][k] += value_jk

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _diag_cov_matrix(weights: np.ndarray, cov_matrix: np.ndarray, min_weight: float, max_weight: float, m: int, n: int) -&gt; None:

        ################################################################################################################

        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

        orders = np.argsort(-eigenvalues)

        order0 = orders[0]
        order1 = orders[1]

        ################################################################################################################

        linspace_x = np.linspace(min_weight, max_weight, m)
        linspace_y = np.linspace(min_weight, max_weight, n)

        for i in range(m):
            c1 = linspace_x[i]

            for j in range(n):
                c2 = linspace_y[j]

                weights[i, j] = (
                    eigenvectors[:, order0] * c1
                    +
                    eigenvectors[:, order1] * c2
                ).astype(weights.dtype)

    ####################################################################################################################

    def train(self, dataset: Union[np.ndarray, Callable], min_weight: float = 0.0, max_weight: float = 1.0) -&gt; None:

        &#34;&#34;&#34;
        Trains the neural network.

        Parameters
        ----------
        dataset : Union[np.ndarray, Callable]
            Training dataset array or generator builder.
        min_weight : float
            Minimum value in the latent space (default: **O.O**)
        max_weight : float
            Maximum value in the latent space (default: **1.O**)
        &#34;&#34;&#34;

        ################################################################################################################

        generator_builder = dataset_to_generator_builder(dataset)

        generator = generator_builder()

        ################################################################################################################

        total_nb = 0

        total_sum = np.zeros((self._dim, ), dtype = np.float64)
        total_prods = np.zeros((self._dim, self._dim, ), dtype = np.float64)

        ################################################################################################################

        for data in generator():

            total_nb += data.shape[0]

            SOM_PCA._update_cov_matrix(total_sum, total_prods, data)

        ################################################################################################################

        total_sum /= total_nb
        total_prods /= total_nb

        ################################################################################################################

        cov_matrix = total_prods - np.outer(total_sum, total_sum)

        ################################################################################################################

        SOM_PCA._diag_cov_matrix(self.get_centroids(), cov_matrix, min_weight, max_weight, self._m, self._n)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>decontamination._algo.som_abstract.SOM_Abstract</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="decontamination.SOM_PCA.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, dataset: Union[np.ndarray, Callable], min_weight: float = 0.0, max_weight: float = 1.0) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the neural network.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Union[np.ndarray, Callable]</code></dt>
<dd>Training dataset array or generator builder.</dd>
<dt><strong><code>min_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum value in the latent space (default: <strong>O.O</strong>)</dd>
<dt><strong><code>max_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Maximum value in the latent space (default: <strong>1.O</strong>)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, dataset: Union[np.ndarray, Callable], min_weight: float = 0.0, max_weight: float = 1.0) -&gt; None:

    &#34;&#34;&#34;
    Trains the neural network.

    Parameters
    ----------
    dataset : Union[np.ndarray, Callable]
        Training dataset array or generator builder.
    min_weight : float
        Minimum value in the latent space (default: **O.O**)
    max_weight : float
        Maximum value in the latent space (default: **1.O**)
    &#34;&#34;&#34;

    ################################################################################################################

    generator_builder = dataset_to_generator_builder(dataset)

    generator = generator_builder()

    ################################################################################################################

    total_nb = 0

    total_sum = np.zeros((self._dim, ), dtype = np.float64)
    total_prods = np.zeros((self._dim, self._dim, ), dtype = np.float64)

    ################################################################################################################

    for data in generator():

        total_nb += data.shape[0]

        SOM_PCA._update_cov_matrix(total_sum, total_prods, data)

    ################################################################################################################

    total_sum /= total_nb
    total_prods /= total_nb

    ################################################################################################################

    cov_matrix = total_prods - np.outer(total_sum, total_sum)

    ################################################################################################################

    SOM_PCA._diag_cov_matrix(self.get_centroids(), cov_matrix, min_weight, max_weight, self._m, self._n)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="decontamination.SOM_Batch"><code class="flex name class">
<span>class <span class="ident">SOM_Batch</span></span>
<span>(</span><span>m: int, n: int, dim: int, dtype: Type[np.float32] = np.float32, topology: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Self Organizing Maps (standard batch implementation).</p>
<p>A rule of thumb to set the size of the grid for a dimensionality reduction
task is that it should contain <span><span class="MathJax_Preview"> 5\sqrt{N} </span><script type="math/tex"> 5\sqrt{N} </script></span> neurons where N is the
number of samples in the dataset to analyze.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron rows.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron columns.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of the input data.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Neural network data type (default: <strong>np.float32</strong>).</dd>
<dt><strong><code>topology</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Topology of the map, either '<strong>square</strong>' or '<strong>hexagonal</strong>' (default: '<strong>hexagonal</strong>').</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SOM_Batch(som_abstract.SOM_Abstract):

    &#34;&#34;&#34;
    Self Organizing Maps (standard batch implementation).
    &#34;&#34;&#34;

    __MODE__ = &#39;batch&#39;

    ####################################################################################################################

    def __init__(self, m: int, n: int, dim: int, dtype: Type[np.single] = np.float32, topology: Optional[str] = None):

        &#34;&#34;&#34;
        A rule of thumb to set the size of the grid for a dimensionality reduction
        task is that it should contain \\( 5\\sqrt{N} \\) neurons where N is the
        number of samples in the dataset to analyze.

        Parameters
        ----------
        m : int
            Number of neuron rows.
        n : int
            Number of neuron columns.
        dim : int
            Dimensionality of the input data.
        dtype : Type[np.single]
            Neural network data type (default: **np.float32**).
        topology : Optional[str]
            Topology of the map, either &#39;**square**&#39; or &#39;**hexagonal**&#39; (default: &#39;**hexagonal**&#39;).
        &#34;&#34;&#34;

        ################################################################################################################

        super().__init__(m, n, dim, dtype, topology)


        ################################################################################################################

        self._n_epochs = None

        self._n_vectors = None

        ################################################################################################################

        self._header_extra = {
            &#39;mode&#39;: &#39;__MODE__&#39;,
            &#39;n_epochs&#39;: &#39;_n_epochs&#39;,
            &#39;n_vectors&#39;: &#39;_n_vectors&#39;,
        }

    ####################################################################################################################

    def train(self, dataset: Union[np.ndarray, Callable], n_epochs: Optional[int] = None, n_vectors: Optional[int] = None, n_error_bins: Optional[int] = 10, show_progress_bar: bool = False) -&gt; None:

        &#34;&#34;&#34;
        Trains the neural network. Use either the &#34;*number of epochs*&#34; training method by specifying `n_epochs` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_epochs}\\}-1 \\)) or the &#34;*number of vectors*&#34; training method by specifying `n_vectors` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_vectors}\\}-1 \\)). A batch formulation of updating weights is implemented: $$ c_i(e)\\equiv\\mathrm{bmu}(x_i,e)\\equiv\\underset{j}{\\mathrm{arg\\,min}}\\lVert x_i-w_j(e)\\rVert $$ $$ n_{ji}(e)=\\left\\{\\begin{array}{ll}1&amp;j=c_i(e)\\\\0&amp;\\mathrm{otherwise}\\end{array}\\right. $$ $$ \\boxed{w_j(e+1)=\\frac{\\sum_{i=0}^{N-1}n_{ji}(e)x_i}{\\sum_{i=0}^{N-1}n_{ji}(e)}} $$ where \\( j=0\\dots m\\times n-1 \\).

        Parameters
        ----------
        dataset : Union[np.ndarray, Callable]
            Training dataset array or generator builder.
        n_epochs : Optional[int]
            Number of epochs to train for (default: **None**).
        n_vectors : Optional[int]
            Number of vectors to train for (default: **None**).
        n_error_bins : int
            Number of error bins (default: **10**).
        show_progress_bar : bool
            Specifies whether to display a progress bar (default: **False**).
        &#34;&#34;&#34;

        ################################################################################################################

        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>decontamination._algo.som_abstract.SOM_Abstract</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="decontamination.SOM_Batch.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, dataset: Union[np.ndarray, Callable], n_epochs: Optional[int] = None, n_vectors: Optional[int] = None, n_error_bins: Optional[int] = 10, show_progress_bar: bool = False) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the neural network. Use either the "<em>number of epochs</em>" training method by specifying <code>n_epochs</code> (then <span><span class="MathJax_Preview"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_epochs}\}-1 </span><script type="math/tex"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_epochs}\}-1 </script></span>) or the "<em>number of vectors</em>" training method by specifying <code>n_vectors</code> (then <span><span class="MathJax_Preview"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_vectors}\}-1 </span><script type="math/tex"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_vectors}\}-1 </script></span>). A batch formulation of updating weights is implemented: <span><span class="MathJax_Preview"> c_i(e)\equiv\mathrm{bmu}(x_i,e)\equiv\underset{j}{\mathrm{arg\,min}}\lVert x_i-w_j(e)\rVert </span><script type="math/tex; mode=display"> c_i(e)\equiv\mathrm{bmu}(x_i,e)\equiv\underset{j}{\mathrm{arg\,min}}\lVert x_i-w_j(e)\rVert </script></span> <span><span class="MathJax_Preview"> n_{ji}(e)=\left\{\begin{array}{ll}1&amp;j=c_i(e)\\0&amp;\mathrm{otherwise}\end{array}\right. </span><script type="math/tex; mode=display"> n_{ji}(e)=\left\{\begin{array}{ll}1&j=c_i(e)\\0&\mathrm{otherwise}\end{array}\right. </script></span> <span><span class="MathJax_Preview"> \boxed{w_j(e+1)=\frac{\sum_{i=0}^{N-1}n_{ji}(e)x_i}{\sum_{i=0}^{N-1}n_{ji}(e)}} </span><script type="math/tex; mode=display"> \boxed{w_j(e+1)=\frac{\sum_{i=0}^{N-1}n_{ji}(e)x_i}{\sum_{i=0}^{N-1}n_{ji}(e)}} </script></span> where <span><span class="MathJax_Preview"> j=0\dots m\times n-1 </span><script type="math/tex"> j=0\dots m\times n-1 </script></span>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Union[np.ndarray, Callable]</code></dt>
<dd>Training dataset array or generator builder.</dd>
<dt><strong><code>n_epochs</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Number of epochs to train for (default: <strong>None</strong>).</dd>
<dt><strong><code>n_vectors</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Number of vectors to train for (default: <strong>None</strong>).</dd>
<dt><strong><code>n_error_bins</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of error bins (default: <strong>10</strong>).</dd>
<dt><strong><code>show_progress_bar</code></strong> :&ensp;<code>bool</code></dt>
<dd>Specifies whether to display a progress bar (default: <strong>False</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, dataset: Union[np.ndarray, Callable], n_epochs: Optional[int] = None, n_vectors: Optional[int] = None, n_error_bins: Optional[int] = 10, show_progress_bar: bool = False) -&gt; None:

    &#34;&#34;&#34;
    Trains the neural network. Use either the &#34;*number of epochs*&#34; training method by specifying `n_epochs` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_epochs}\\}-1 \\)) or the &#34;*number of vectors*&#34; training method by specifying `n_vectors` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_vectors}\\}-1 \\)). A batch formulation of updating weights is implemented: $$ c_i(e)\\equiv\\mathrm{bmu}(x_i,e)\\equiv\\underset{j}{\\mathrm{arg\\,min}}\\lVert x_i-w_j(e)\\rVert $$ $$ n_{ji}(e)=\\left\\{\\begin{array}{ll}1&amp;j=c_i(e)\\\\0&amp;\\mathrm{otherwise}\\end{array}\\right. $$ $$ \\boxed{w_j(e+1)=\\frac{\\sum_{i=0}^{N-1}n_{ji}(e)x_i}{\\sum_{i=0}^{N-1}n_{ji}(e)}} $$ where \\( j=0\\dots m\\times n-1 \\).

    Parameters
    ----------
    dataset : Union[np.ndarray, Callable]
        Training dataset array or generator builder.
    n_epochs : Optional[int]
        Number of epochs to train for (default: **None**).
    n_vectors : Optional[int]
        Number of vectors to train for (default: **None**).
    n_error_bins : int
        Number of error bins (default: **10**).
    show_progress_bar : bool
        Specifies whether to display a progress bar (default: **False**).
    &#34;&#34;&#34;

    ################################################################################################################

    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="decontamination.SOM_Online"><code class="flex name class">
<span>class <span class="ident">SOM_Online</span></span>
<span>(</span><span>m: int, n: int, dim: int, dtype: Type[np.float32] = np.float32, topology: Optional[str] = None, alpha: float = None, sigma: float = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Self Organizing Maps (standard online implementation).</p>
<p>A rule of thumb to set the size of the grid for a dimensionality reduction
task is that it should contain <span><span class="MathJax_Preview"> 5\sqrt{N} </span><script type="math/tex"> 5\sqrt{N} </script></span> neurons where N is the
number of samples in the dataset to analyze.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron rows.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron columns.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of the input data.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Neural network data type (default: <strong>np.float32</strong>).</dd>
<dt><strong><code>topology</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Topology of the map, either '<strong>square</strong>' or '<strong>hexagonal</strong>' (default: '<strong>hexagonal</strong>').</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Starting value of the learning rate (default: <strong>0.3</strong>).</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>Starting value of the neighborhood radius (default: <span><span class="MathJax_Preview"> \mathrm{max}(m,n)/2 </span><script type="math/tex"> \mathrm{max}(m,n)/2 </script></span>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SOM_Online(som_abstract.SOM_Abstract):

    &#34;&#34;&#34;
    Self Organizing Maps (standard online implementation).
    &#34;&#34;&#34;

    __MODE__ = &#39;online&#39;

    ####################################################################################################################

    def __init__(self, m: int, n: int, dim: int, dtype: Type[np.single] = np.float32, topology: Optional[str] = None, alpha: float = None, sigma: float = None):

        &#34;&#34;&#34;
        A rule of thumb to set the size of the grid for a dimensionality reduction
        task is that it should contain \\( 5\\sqrt{N} \\) neurons where N is the
        number of samples in the dataset to analyze.

        Parameters
        ----------
        m : int
            Number of neuron rows.
        n : int
            Number of neuron columns.
        dim : int
            Dimensionality of the input data.
        dtype : Type[np.single]
            Neural network data type (default: **np.float32**).
        topology : Optional[str]
            Topology of the map, either &#39;**square**&#39; or &#39;**hexagonal**&#39; (default: &#39;**hexagonal**&#39;).
        alpha : float
            Starting value of the learning rate (default: **0.3**).
        sigma : float
            Starting value of the neighborhood radius (default: \\( \\mathrm{max}(m,n)/2 \\)).
        &#34;&#34;&#34;

        ################################################################################################################

        super().__init__(m, n, dim, dtype, topology)

        ################################################################################################################

        self._alpha = 0.3 if alpha is None else float(alpha)

        self._sigma = max(m, n) / 2.0 if sigma is None else float(sigma)

        ################################################################################################################

        self._n_epochs = None

        self._n_vectors = None

        ################################################################################################################

        self._header_extra = {
            &#39;mode&#39;: &#39;__MODE__&#39;,
            &#39;alpha&#39;: &#39;_alpha&#39;,
            &#39;sigma&#39;: &#39;_sigma&#39;,
            &#39;n_epochs&#39;: &#39;_n_epochs&#39;,
            &#39;n_vectors&#39;: &#39;_n_vectors&#39;,
        }

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _train_step1_epoch(weights: np.ndarray, quantization_errors: np.ndarray, topographic_errors: np.ndarray, topography: np.ndarray, data: np.ndarray, cur_epoch: int, n_epochs: int, alpha0: float, sigma0: float, penalty_dist: float, mn: int):

        ################################################################################################################

        decay_function = asymptotic_decay(cur_epoch, n_epochs)

        alpha = alpha0 * decay_function

        sigma = sigma0 * decay_function

        ################################################################################################################

        for i in range(data.shape[0]):

            quantization_error, topographic_error = _train_step2(weights, topography, data[i], alpha, sigma, penalty_dist, mn)

            quantization_errors[cur_epoch] += quantization_error
            topographic_errors[cur_epoch] += topographic_error

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _train_step1_iter(weights: np.ndarray, quantization_errors: np.ndarray, topographic_errors: np.ndarray, topography: np.ndarray, data: np.ndarray, cur_vector: int, n_vectors: int, n_err_bins: int, alpha0: float, sigma0: float, penalty_dist: float, mn: int):

        for i in range(data.shape[0]):

            ############################################################################################################

            decay_function = asymptotic_decay(cur_vector + i, n_vectors)

            alpha = alpha0 * decay_function

            sigma = sigma0 * decay_function

            ############################################################################################################

            quantization_error, topographic_error = _train_step2(weights, topography, data[i], alpha, sigma, penalty_dist, mn)

            cur_err_bin = (n_err_bins * (cur_vector + i)) // n_vectors

            quantization_errors[cur_err_bin] += quantization_error
            topographic_errors[cur_err_bin] += topographic_error

    ####################################################################################################################

    def train(self, dataset: Union[np.ndarray, Callable], n_epochs: Optional[int] = None, n_vectors: Optional[int] = None, n_error_bins: Optional[int] = 10, show_progress_bar: bool = False) -&gt; None:

        &#34;&#34;&#34;
        Trains the neural network. Use either the &#34;*number of epochs*&#34; training method by specifying `n_epochs` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_epochs}\\}-1 \\)) or the &#34;*number of vectors*&#34; training method by specifying `n_vectors` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_vectors}\\}-1 \\)). An online formulation of updating weights is implemented: $$ c_i(e)\\equiv\\mathrm{bmu}(x_i,e)\\equiv\\underset{j}{\\mathrm{arg\\,min}}\\lVert x_i-w_j(e)\\rVert $$ $$ \\Theta_{ji}(e)\\equiv\\alpha(e)\\cdot\\exp\\left(-\\frac{\\lVert j-c_i\\rVert}{2\\sigma^2(e)}\\right) $$ $$ \\boxed{\\mathrm{iteratively\\,for}\\,i=0\\dots N-1\\,\\mathrm{:}\\,w_j(e+1)=w_j(e)+\\Theta_{ji}(e)[x_i-w_j(e)]} $$ where \\( j=0\\dots m\\times n-1 \\) and, at epoch \\( e \\), \\( \\alpha(e)\\equiv\\alpha\\cdot\\frac{1}{1+2\\frac{e}{e_\\mathrm{tot}}} \\) is the learning rate and \\( \\sigma(e)\\equiv\\sigma\\cdot\\frac{1}{1+2\\frac{e}{e_\\mathrm{tot}}} \\) is the neighborhood radius.

        Parameters
        ----------
        dataset : Union[np.ndarray, Callable]
            Training dataset array or generator builder.
        n_epochs : Optional[int]
            Number of epochs to train for (default: **None**).
        n_vectors : Optional[int]
            Number of vectors to train for (default: **None**).
        n_error_bins : int
            Number of quantization and topographic error bins (default: **10**).
        show_progress_bar : bool
            Specifies whether to display a progress bar (default: **False**).
        &#34;&#34;&#34;

        ################################################################################################################

        generator_builder = dataset_to_generator_builder(dataset)

        ################################################################################################################

        cur_vector = 0

        self._n_epochs = n_epochs

        self._n_vectors = n_vectors

        penalty_dist = 2.0 if self._topology == &#39;square&#39; else 1.0

        if not (n_epochs is None) and (n_vectors is None):

            ############################################################################################################
            # TRAINING BY NUMBER OF EPOCHS                                                                             #
            ############################################################################################################

            self._quantization_errors = np.zeros(n_epochs, dtype = np.float32)

            self._topographic_errors = np.zeros(n_epochs, dtype = np.float32)

            ############################################################################################################

            for cur_epoch in tqdm.trange(n_epochs, disable = not show_progress_bar):

                generator = generator_builder()

                for data in generator():

                    cur_vector += data.shape[0]

                    SOM_Online._train_step1_epoch(
                        self._weights,
                        self._quantization_errors,
                        self._topographic_errors,
                        self._topography,
                        data,
                        cur_epoch,
                        n_epochs,
                        self._alpha,
                        self._sigma,
                        penalty_dist,
                        self._m * self._n
                    )

            ############################################################################################################

            if cur_vector &gt; 0:

                self._quantization_errors = self._quantization_errors * n_epochs / cur_vector

                self._topographic_errors = self._topographic_errors * n_epochs / cur_vector

            ############################################################################################################

        elif (n_epochs is None) and not (n_vectors is None):

            ############################################################################################################
            # TRAINING BY NUMBER OF VECTORS                                                                            #
            ############################################################################################################

            self._quantization_errors = np.zeros(n_error_bins, dtype = np.float32)

            self._topographic_errors = np.zeros(n_error_bins, dtype = np.float32)

            ############################################################################################################

            progress_bar = tqdm.tqdm(total = n_vectors, disable = not show_progress_bar)

            generator = generator_builder()

            for data in generator():

                count = min(data.shape[0], n_vectors - cur_vector)

                SOM_Online._train_step1_iter(
                    self._weights,
                    self._quantization_errors,
                    self._topographic_errors,
                    self._topography,
                    data[0: count],
                    cur_vector,
                    n_vectors,
                    n_error_bins,
                    self._alpha,
                    self._sigma,
                    penalty_dist,
                    self._m * self._n
                )

                cur_vector += count

                progress_bar.update(count)

                if cur_vector &gt;= n_vectors:

                    break

            ############################################################################################################

            if cur_vector &gt; 0:

                self._quantization_errors = self._quantization_errors * n_error_bins / cur_vector

                self._topographic_errors = self._topographic_errors * n_error_bins / cur_vector

            ############################################################################################################

        else:

            raise Exception(&#39;Invalid training method, specify either `n_epochs` or `n_vectors`.&#39;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>decontamination._algo.som_abstract.SOM_Abstract</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="decontamination.SOM_Online.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, dataset: Union[np.ndarray, Callable], n_epochs: Optional[int] = None, n_vectors: Optional[int] = None, n_error_bins: Optional[int] = 10, show_progress_bar: bool = False) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the neural network. Use either the "<em>number of epochs</em>" training method by specifying <code>n_epochs</code> (then <span><span class="MathJax_Preview"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_epochs}\}-1 </span><script type="math/tex"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_epochs}\}-1 </script></span>) or the "<em>number of vectors</em>" training method by specifying <code>n_vectors</code> (then <span><span class="MathJax_Preview"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_vectors}\}-1 </span><script type="math/tex"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_vectors}\}-1 </script></span>). An online formulation of updating weights is implemented: <span><span class="MathJax_Preview"> c_i(e)\equiv\mathrm{bmu}(x_i,e)\equiv\underset{j}{\mathrm{arg\,min}}\lVert x_i-w_j(e)\rVert </span><script type="math/tex; mode=display"> c_i(e)\equiv\mathrm{bmu}(x_i,e)\equiv\underset{j}{\mathrm{arg\,min}}\lVert x_i-w_j(e)\rVert </script></span> <span><span class="MathJax_Preview"> \Theta_{ji}(e)\equiv\alpha(e)\cdot\exp\left(-\frac{\lVert j-c_i\rVert}{2\sigma^2(e)}\right) </span><script type="math/tex; mode=display"> \Theta_{ji}(e)\equiv\alpha(e)\cdot\exp\left(-\frac{\lVert j-c_i\rVert}{2\sigma^2(e)}\right) </script></span> <span><span class="MathJax_Preview"> \boxed{\mathrm{iteratively\,for}\,i=0\dots N-1\,\mathrm{:}\,w_j(e+1)=w_j(e)+\Theta_{ji}(e)[x_i-w_j(e)]} </span><script type="math/tex; mode=display"> \boxed{\mathrm{iteratively\,for}\,i=0\dots N-1\,\mathrm{:}\,w_j(e+1)=w_j(e)+\Theta_{ji}(e)[x_i-w_j(e)]} </script></span> where <span><span class="MathJax_Preview"> j=0\dots m\times n-1 </span><script type="math/tex"> j=0\dots m\times n-1 </script></span> and, at epoch <span><span class="MathJax_Preview"> e </span><script type="math/tex"> e </script></span>, <span><span class="MathJax_Preview"> \alpha(e)\equiv\alpha\cdot\frac{1}{1+2\frac{e}{e_\mathrm{tot}}} </span><script type="math/tex"> \alpha(e)\equiv\alpha\cdot\frac{1}{1+2\frac{e}{e_\mathrm{tot}}} </script></span> is the learning rate and <span><span class="MathJax_Preview"> \sigma(e)\equiv\sigma\cdot\frac{1}{1+2\frac{e}{e_\mathrm{tot}}} </span><script type="math/tex"> \sigma(e)\equiv\sigma\cdot\frac{1}{1+2\frac{e}{e_\mathrm{tot}}} </script></span> is the neighborhood radius.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Union[np.ndarray, Callable]</code></dt>
<dd>Training dataset array or generator builder.</dd>
<dt><strong><code>n_epochs</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Number of epochs to train for (default: <strong>None</strong>).</dd>
<dt><strong><code>n_vectors</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Number of vectors to train for (default: <strong>None</strong>).</dd>
<dt><strong><code>n_error_bins</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quantization and topographic error bins (default: <strong>10</strong>).</dd>
<dt><strong><code>show_progress_bar</code></strong> :&ensp;<code>bool</code></dt>
<dd>Specifies whether to display a progress bar (default: <strong>False</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, dataset: Union[np.ndarray, Callable], n_epochs: Optional[int] = None, n_vectors: Optional[int] = None, n_error_bins: Optional[int] = 10, show_progress_bar: bool = False) -&gt; None:

    &#34;&#34;&#34;
    Trains the neural network. Use either the &#34;*number of epochs*&#34; training method by specifying `n_epochs` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_epochs}\\}-1 \\)) or the &#34;*number of vectors*&#34; training method by specifying `n_vectors` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_vectors}\\}-1 \\)). An online formulation of updating weights is implemented: $$ c_i(e)\\equiv\\mathrm{bmu}(x_i,e)\\equiv\\underset{j}{\\mathrm{arg\\,min}}\\lVert x_i-w_j(e)\\rVert $$ $$ \\Theta_{ji}(e)\\equiv\\alpha(e)\\cdot\\exp\\left(-\\frac{\\lVert j-c_i\\rVert}{2\\sigma^2(e)}\\right) $$ $$ \\boxed{\\mathrm{iteratively\\,for}\\,i=0\\dots N-1\\,\\mathrm{:}\\,w_j(e+1)=w_j(e)+\\Theta_{ji}(e)[x_i-w_j(e)]} $$ where \\( j=0\\dots m\\times n-1 \\) and, at epoch \\( e \\), \\( \\alpha(e)\\equiv\\alpha\\cdot\\frac{1}{1+2\\frac{e}{e_\\mathrm{tot}}} \\) is the learning rate and \\( \\sigma(e)\\equiv\\sigma\\cdot\\frac{1}{1+2\\frac{e}{e_\\mathrm{tot}}} \\) is the neighborhood radius.

    Parameters
    ----------
    dataset : Union[np.ndarray, Callable]
        Training dataset array or generator builder.
    n_epochs : Optional[int]
        Number of epochs to train for (default: **None**).
    n_vectors : Optional[int]
        Number of vectors to train for (default: **None**).
    n_error_bins : int
        Number of quantization and topographic error bins (default: **10**).
    show_progress_bar : bool
        Specifies whether to display a progress bar (default: **False**).
    &#34;&#34;&#34;

    ################################################################################################################

    generator_builder = dataset_to_generator_builder(dataset)

    ################################################################################################################

    cur_vector = 0

    self._n_epochs = n_epochs

    self._n_vectors = n_vectors

    penalty_dist = 2.0 if self._topology == &#39;square&#39; else 1.0

    if not (n_epochs is None) and (n_vectors is None):

        ############################################################################################################
        # TRAINING BY NUMBER OF EPOCHS                                                                             #
        ############################################################################################################

        self._quantization_errors = np.zeros(n_epochs, dtype = np.float32)

        self._topographic_errors = np.zeros(n_epochs, dtype = np.float32)

        ############################################################################################################

        for cur_epoch in tqdm.trange(n_epochs, disable = not show_progress_bar):

            generator = generator_builder()

            for data in generator():

                cur_vector += data.shape[0]

                SOM_Online._train_step1_epoch(
                    self._weights,
                    self._quantization_errors,
                    self._topographic_errors,
                    self._topography,
                    data,
                    cur_epoch,
                    n_epochs,
                    self._alpha,
                    self._sigma,
                    penalty_dist,
                    self._m * self._n
                )

        ############################################################################################################

        if cur_vector &gt; 0:

            self._quantization_errors = self._quantization_errors * n_epochs / cur_vector

            self._topographic_errors = self._topographic_errors * n_epochs / cur_vector

        ############################################################################################################

    elif (n_epochs is None) and not (n_vectors is None):

        ############################################################################################################
        # TRAINING BY NUMBER OF VECTORS                                                                            #
        ############################################################################################################

        self._quantization_errors = np.zeros(n_error_bins, dtype = np.float32)

        self._topographic_errors = np.zeros(n_error_bins, dtype = np.float32)

        ############################################################################################################

        progress_bar = tqdm.tqdm(total = n_vectors, disable = not show_progress_bar)

        generator = generator_builder()

        for data in generator():

            count = min(data.shape[0], n_vectors - cur_vector)

            SOM_Online._train_step1_iter(
                self._weights,
                self._quantization_errors,
                self._topographic_errors,
                self._topography,
                data[0: count],
                cur_vector,
                n_vectors,
                n_error_bins,
                self._alpha,
                self._sigma,
                penalty_dist,
                self._m * self._n
            )

            cur_vector += count

            progress_bar.update(count)

            if cur_vector &gt;= n_vectors:

                break

        ############################################################################################################

        if cur_vector &gt; 0:

            self._quantization_errors = self._quantization_errors * n_error_bins / cur_vector

            self._topographic_errors = self._topographic_errors * n_error_bins / cur_vector

        ############################################################################################################

    else:

        raise Exception(&#39;Invalid training method, specify either `n_epochs` or `n_vectors`.&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="decontamination.device_array_from" href="#decontamination.device_array_from">device_array_from</a></code></li>
<li><code><a title="decontamination.device_array_empty" href="#decontamination.device_array_empty">device_array_empty</a></code></li>
<li><code><a title="decontamination.device_array_zeros" href="#decontamination.device_array_zeros">device_array_zeros</a></code></li>
<li><code><a title="decontamination.device_array_full" href="#decontamination.device_array_full">device_array_full</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="decontamination.jit" href="#decontamination.jit">jit</a></code></h4>
</li>
<li>
<h4><code><a title="decontamination.DeviceArray" href="#decontamination.DeviceArray">DeviceArray</a></code></h4>
<ul class="">
<li><code><a title="decontamination.DeviceArray.copy_to_host" href="#decontamination.DeviceArray.copy_to_host">copy_to_host</a></code></li>
<li><code><a title="decontamination.DeviceArray.shape" href="#decontamination.DeviceArray.shape">shape</a></code></li>
<li><code><a title="decontamination.DeviceArray.dtype" href="#decontamination.DeviceArray.dtype">dtype</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="decontamination.SOM_Abstract" href="#decontamination.SOM_Abstract">SOM_Abstract</a></code></h4>
<ul class="">
<li><code><a title="decontamination.SOM_Abstract.init_rand" href="#decontamination.SOM_Abstract.init_rand">init_rand</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.init_from" href="#decontamination.SOM_Abstract.init_from">init_from</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.save" href="#decontamination.SOM_Abstract.save">save</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.load" href="#decontamination.SOM_Abstract.load">load</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_weights" href="#decontamination.SOM_Abstract.get_weights">get_weights</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_centroids" href="#decontamination.SOM_Abstract.get_centroids">get_centroids</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_quantization_errors" href="#decontamination.SOM_Abstract.get_quantization_errors">get_quantization_errors</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_topographic_errors" href="#decontamination.SOM_Abstract.get_topographic_errors">get_topographic_errors</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_distance_map" href="#decontamination.SOM_Abstract.get_distance_map">get_distance_map</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_activation_map" href="#decontamination.SOM_Abstract.get_activation_map">get_activation_map</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_winners" href="#decontamination.SOM_Abstract.get_winners">get_winners</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="decontamination.SOM_PCA" href="#decontamination.SOM_PCA">SOM_PCA</a></code></h4>
<ul class="">
<li><code><a title="decontamination.SOM_PCA.train" href="#decontamination.SOM_PCA.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="decontamination.SOM_Batch" href="#decontamination.SOM_Batch">SOM_Batch</a></code></h4>
<ul class="">
<li><code><a title="decontamination.SOM_Batch.train" href="#decontamination.SOM_Batch.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="decontamination.SOM_Online" href="#decontamination.SOM_Online">SOM_Online</a></code></h4>
<ul class="">
<li><code><a title="decontamination.SOM_Online.train" href="#decontamination.SOM_Online.train">train</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>