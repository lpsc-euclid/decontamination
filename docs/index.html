<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>decontamination API documentation</title>
<meta name="description" content="[![][License img]][License]
&lt;span style=&#34;margin-right: 0.5rem;&#34;&gt;&lt;/span&gt;
[![][MainRepo img]][MainRepo]
&lt;!--
&lt;span style=&#34;margin-right:
â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>decontamination</code></h1>
</header>
<section id="section-intro">
<p><a href="http://www.cecill.info/licences/Licence_CeCILL-C_V1-en.txt"><img alt="" src="https://img.shields.io/badge/license-CeCILL--C-blue.svg"></a>
<span style="margin-right: 0.5rem;"></span>
<a href="https://gitlab.in2p3.fr/lpsc-euclid/decontamination"><img alt="" src="https://img.shields.io/badge/Main%20Repo-gitlab.in2p3.fr-success"></a></p>
<!--
<span style="margin-right: 0.5rem;"></span>
[![][AltRepo img]][AltRepo]
-->
<p><a href="http://lpsc.in2p3.fr/"
target="_blank"><img src="logo_lpsc.svg" alt="LPSC" height="72" /></a>
<span style="margin-right: 2.0rem;"></span>
<a href="https://www.ijclab.in2p3.fr/"
target="_blank"><img src="logo_ijclab.svg" alt="IJCLab" height="72" /></a>
<span style="margin-right: 2.0rem;"></span>
<a href="http://www.in2p3.fr/"
target="_blank"><img src="logo_in2p3.svg" alt="IN2P3" height="72" /></a>
<span style="margin-right: 2.0rem;"></span>
<a href="http://www.univ-grenoble-alpes.fr/" target="_blank"><img src="logo_uga.svg" alt="UGA" height="72" /></a></p>
<p>A toolbox for performing systematics decontamination in cosmology analyses.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
########################################################################################################################

&#34;&#34;&#34;
.. include:: ../docs/header.md

A toolbox for performing systematics decontamination in cosmology analyses.
&#34;&#34;&#34;

########################################################################################################################

import numpy as np

########################################################################################################################
# JIT                                                                                                                  #
########################################################################################################################

from ._jit import CPU_OPTIMIZATION_AVAILABLE, GPU_OPTIMIZATION_AVAILABLE, jit

from ._jit import DeviceArray, device_array_from, device_array_empty, device_array_zeros, device_array_full

########################################################################################################################
# ALGO                                                                                                                 #
########################################################################################################################

from ._algo.som_abstract import SOM_Abstract

from ._algo.som_pca import SOM_PCA

from ._algo.som_batch import SOM_Batch

from ._algo.som_online import SOM_Online

from ._algo.clustering import Clustering

########################################################################################################################
# PLOTTING                                                                                                             #
########################################################################################################################

from ._plotting.latent_space import display_latent_space
from ._plotting.clustering import display_clusters

########################################################################################################################
# UTILITIES                                                                                                            #
########################################################################################################################

def array_to_string(arr):

    s = np.array2string(arr, separator = &#39;, &#39;, suppress_small = True)

    return s.replace(&#39;[ &#39;, &#39;[&#39;).replace(&#39; ]&#39;, &#39;]&#39;)

########################################################################################################################
# EXPORTS                                                                                                              #
########################################################################################################################

__all__ = [
    &#39;CPU_OPTIMIZATION_AVAILABLE&#39;, &#39;GPU_OPTIMIZATION_AVAILABLE&#39;, &#39;jit&#39;,
    &#39;DeviceArray&#39;, &#39;device_array_from&#39;, &#39;device_array_empty&#39;, &#39;device_array_zeros&#39;, &#39;device_array_full&#39;,
    &#39;SOM_Abstract&#39;, &#39;SOM_PCA&#39;, &#39;SOM_Batch&#39;, &#39;SOM_Online&#39;,
    &#39;Clustering&#39;,
    &#39;display_latent_space&#39;, &#39;display_clusters&#39;
]

########################################################################################################################</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="decontamination.device_array_from"><code class="name flex">
<span>def <span class="ident">device_array_from</span></span>(<span>array:Â np.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>New device array (see <code><a title="decontamination.DeviceArray" href="#decontamination.DeviceArray">DeviceArray</a></code>), initialized from a Numpy ndarray.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_array_from(array: np.ndarray):

    &#34;&#34;&#34;
    New device array (see `DeviceArray`), initialized from a Numpy ndarray.
    &#34;&#34;&#34;

    return DeviceArray(array.shape, array.dtype, content = array)</code></pre>
</details>
</dd>
<dt id="decontamination.device_array_empty"><code class="name flex">
<span>def <span class="ident">device_array_empty</span></span>(<span>shape:Â Union[tuple,Â int], dtype:Â Type[np.float32]Â =Â np.float32)</span>
</code></dt>
<dd>
<div class="desc"><p>New device array (see <code><a title="decontamination.DeviceArray" href="#decontamination.DeviceArray">DeviceArray</a></code>), not initialized. Similar to <code>np.empty()</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>Union[tuple, int]</code></dt>
<dd>Desired shape for the new array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Desired data-type for the new array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_array_empty(shape: Union[tuple, int], dtype: Type[np.single] = np.float32):

    &#34;&#34;&#34;
    New device array (see `DeviceArray`), not initialized. Similar to `np.empty()`.

    Parameters
    ----------
    shape : Union[tuple, int]
        Desired shape for the new array.
    dtype : Type[np.single]
        Desired data-type for the new array.
    &#34;&#34;&#34;

    return DeviceArray(shape, dtype, content = None)</code></pre>
</details>
</dd>
<dt id="decontamination.device_array_zeros"><code class="name flex">
<span>def <span class="ident">device_array_zeros</span></span>(<span>shape:Â Union[tuple,Â int], dtype:Â Type[np.float32]Â =Â np.float32)</span>
</code></dt>
<dd>
<div class="desc"><p>New device array (see <code><a title="decontamination.DeviceArray" href="#decontamination.DeviceArray">DeviceArray</a></code>), filled with <strong>0</strong>. Similar to <code>np.zeros()</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>Union[tuple, int]</code></dt>
<dd>Desired shape for the new array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Desired data-type for the new array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_array_zeros(shape: Union[tuple, int], dtype: Type[np.single] = np.float32):

    &#34;&#34;&#34;
    New device array (see `DeviceArray`), filled with **0**. Similar to `np.zeros()`.

    Parameters
    ----------
    shape : Union[tuple, int]
        Desired shape for the new array.
    dtype : Type[np.single]
        Desired data-type for the new array.
    &#34;&#34;&#34;

    return DeviceArray(shape, dtype, content = 0)</code></pre>
</details>
</dd>
<dt id="decontamination.device_array_full"><code class="name flex">
<span>def <span class="ident">device_array_full</span></span>(<span>shape:Â Union[tuple,Â int], value:Â Union[int,Â float], dtype:Â Type[np.float32]Â =Â np.float32)</span>
</code></dt>
<dd>
<div class="desc"><p>New device array (see <code><a title="decontamination.DeviceArray" href="#decontamination.DeviceArray">DeviceArray</a></code>), filled with <strong>value</strong>. Similar to <code>np.full()</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>Union[tuple, int]</code></dt>
<dd>Desired shape for the new array.</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>Union[int, float]</code></dt>
<dd>Desired value for the new array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Desired data-type for the new array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_array_full(shape: Union[tuple, int], value: Union[int, float], dtype: Type[np.single] = np.float32):

    &#34;&#34;&#34;
    New device array (see `DeviceArray`), filled with **value**. Similar to `np.full()`.

    Parameters
    ----------
    shape : Union[tuple, int]
        Desired shape for the new array.
    value : Union[int, float]
        Desired value for the new array.
    dtype : Type[np.single]
        Desired data-type for the new array.
    &#34;&#34;&#34;

    return DeviceArray(shape, dtype, content = value)</code></pre>
</details>
</dd>
<dt id="decontamination.display_latent_space"><code class="name flex">
<span>def <span class="ident">display_latent_space</span></span>(<span>weights:Â np.ndarray, topology:Â Optional[str]Â =Â None, v_min:Â floatÂ =Â None, v_max:Â floatÂ =Â None, cmap:Â strÂ =Â 'viridis', log_scale:Â boolÂ =Â False, antialiased:Â boolÂ =Â False, show_frame:Â boolÂ =Â True, show_colorbar:Â boolÂ =Â True, show_histogram:Â boolÂ =Â True, n_hist_bins:Â intÂ =Â 100, cluster_ids:Â Optional[np.ndarray]Â =Â None) â€‘>Â Tuple[plt.Figure,Â plt.Axes]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>weights</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Weights of the map.</dd>
<dt><strong><code>topology</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Topology of the map, either <strong>'square'</strong> or <strong>'hexagonal'</strong> (default: <strong>None</strong>, uses: <strong>'hexagonal'</strong>).</dd>
<dt><strong><code>cmap</code></strong> :&ensp;<code>str</code></dt>
<dd>Color map (default: <strong>'viridis'</strong>).</dd>
<dt><strong><code>v_min</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum color scale (default: <strong>None</strong>, uses: min(data)).</dd>
<dt><strong><code>v_max</code></strong> :&ensp;<code>float</code></dt>
<dd>Maximum color scale (default: <strong>None</strong>, uses: max(data)).</dd>
<dt><strong><code>log_scale</code></strong> :&ensp;<code>bool</code></dt>
<dd>Specifies whether to enable the logarithm scaling (default: <strong>False</strong>).</dd>
<dt><strong><code>antialiased</code></strong> :&ensp;<code>bool</code></dt>
<dd>Specifies whether to enable the antialiasing (default: <strong>False</strong>).</dd>
<dt><strong><code>show_frame</code></strong> :&ensp;<code>bool</code></dt>
<dd>Specifies whether to display the frame (default: <strong>True</strong>).</dd>
<dt><strong><code>show_colorbar</code></strong> :&ensp;<code>bool</code></dt>
<dd>Specifies whether to display the colorbar (default: <strong>True</strong>).</dd>
<dt><strong><code>show_histogram</code></strong> :&ensp;<code>bool</code></dt>
<dd>Specifies whether to display the histogram (default: <strong>True</strong>).</dd>
<dt><strong><code>n_hist_bins</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of histogram bins in the colorbar (default: <strong>100</strong>).</dd>
<dt><strong><code>cluster_ids</code></strong> :&ensp;<code>Optional[np.ndarray]</code></dt>
<dd>Array of cluster identifiers (see <code><a title="decontamination.Clustering" href="#decontamination.Clustering">Clustering</a></code>, default: <strong>None</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def display_latent_space(weights: np.ndarray, topology: Optional[str] = None, v_min: float = None, v_max: float = None, cmap: str = &#39;viridis&#39;, log_scale: bool = False, antialiased: bool = False, show_frame: bool = True, show_colorbar: bool = True, show_histogram: bool = True, n_hist_bins: int = 100, cluster_ids: Optional[np.ndarray] = None) -&gt; Tuple[plt.Figure, plt.Axes]:

    &#34;&#34;&#34;
    Parameters
    ----------
    weights : np.ndarray
        Weights of the map.
    topology : Optional[str]
        Topology of the map, either **&#39;square&#39;** or **&#39;hexagonal&#39;** (default: **None**, uses: **&#39;hexagonal&#39;**).
    cmap : str
        Color map (default: **&#39;viridis&#39;**).
    v_min : float
        Minimum color scale (default: **None**, uses: min(data)).
    v_max : float
        Maximum color scale (default: **None**, uses: max(data)).
    log_scale : bool
        Specifies whether to enable the logarithm scaling (default: **False**).
    antialiased : bool
        Specifies whether to enable the antialiasing (default: **False**).
    show_frame : bool
        Specifies whether to display the frame (default: **True**).
    show_colorbar : bool
        Specifies whether to display the colorbar (default: **True**).
    show_histogram : bool
        Specifies whether to display the histogram (default: **True**).
    n_hist_bins : int
        Number of histogram bins in the colorbar (default: **100**).
    cluster_ids : Optional[np.ndarray]
        Array of cluster identifiers (see `Clustering`, default: **None**).
    &#34;&#34;&#34;

    ####################################################################################################################

    if len(weights.shape) != 2:

        raise ValueError(&#39;Invalid latent space shape, must be (m, n)&#39;)

    ####################################################################################################################

    fig, ax, v_min, v_max, norm, cmap = _init_plot(weights, v_min, v_max, cmap, log_scale)

    ####################################################################################################################

    if max(weights.shape) &gt; 200:

        _display_latent_space_big(ax, weights, cmap, norm)

    else:

        if topology == &#39;square&#39;:

            _display_latent_space_square(ax, weights, cmap, norm, antialiased)

        else:

            _display_latent_space_hexagonal(ax, weights, cmap, norm, antialiased)

    ####################################################################################################################

    if show_colorbar:

        _build_colorbar(ax, weights, v_min, v_max, cmap, norm, log_scale, show_histogram, n_hist_bins)

    ####################################################################################################################

    if cluster_ids is not None:

        clustering.display_clusters(ax, cluster_ids.reshape(weights.shape[0], weights.shape[1]), topology = topology)

    ####################################################################################################################

    ax.set_frame_on(show_frame)

    ax.set_aspect(&#39;equal&#39;)

    ax.invert_yaxis()

    ####################################################################################################################

    return fig, ax</code></pre>
</details>
</dd>
<dt id="decontamination.display_clusters"><code class="name flex">
<span>def <span class="ident">display_clusters</span></span>(<span>ax:Â plt.Axes, cluster_ids:Â np.ndarray, topology:Â Optional[str]Â =Â None) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ax</code></strong> :&ensp;<code>pyplot.Axes</code></dt>
<dd>Matplotlib <code>Axes</code> object.</dd>
<dt><strong><code>cluster_ids</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array of cluster identifiers.</dd>
<dt><strong><code>topology</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Topology of the map, either <strong>'square'</strong> or <strong>'hexagonal'</strong> (default: <strong>None</strong>, uses: <strong>'hexagonal'</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def display_clusters(ax: pyplot.Axes, cluster_ids: np.ndarray, topology: Optional[str] = None) -&gt; None:

    &#34;&#34;&#34;
    Parameters
    ----------
    ax : pyplot.Axes
        Matplotlib `Axes` object.
    cluster_ids : np.ndarray
        Array of cluster identifiers.
    topology : Optional[str]
        Topology of the map, either **&#39;square&#39;** or **&#39;hexagonal&#39;** (default: **None**, uses: **&#39;hexagonal&#39;**).
    &#34;&#34;&#34;

    if max(cluster_ids.shape[0], cluster_ids.shape[1]) &gt; 200 or topology == &#39;square&#39;:

        display_clusters_square(ax, cluster_ids)

    else:

        display_clusters_hexagonal(ax, cluster_ids)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="decontamination.jit"><code class="flex name class">
<span>class <span class="ident">jit</span></span>
<span>(</span><span>kernel:Â boolÂ =Â False, fastmath:Â boolÂ =Â False, parallel:Â boolÂ =Â False)</span>
</code></dt>
<dd>
<div class="desc"><p>Decorator to recompile Python functions into native CPU/GPU ones.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>kernel</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicates whether this function is a CPU/GPU kernel (default: <strong>False</strong>).</dd>
<dt><strong><code>fastmath</code></strong> :&ensp;<code>bool</code></dt>
<dd>Enables fast-math optimizations when running on CPU (default: <strong>False</strong>).</dd>
<dt><strong><code>parallel</code></strong> :&ensp;<code>bool</code></dt>
<dd>Enables automatic parallelization when running on CPU (default: <strong>False</strong>).</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>@jit(parallel = False)
def foo_xpu(a, b):

    return a + b

@jit(kernel = True)
def foo_kernel(result, a, b):

    ########################################################################
    # !--BEGIN-CPU--

    for i in range(result.shape[0]):

        result[i] = foo_xpu(a[i], b[i])

    # !--END-CPU--
    ########################################################################
    # !--BEGIN-GPU--

    i = jit.grid(1)
    if i &lt; result.shape[0]:

        result[i] = foo_xpu(a[i], b[i])

    # !--END-GPU--
    ########################################################################

use_gpu = True
threads_per_block = 32

A = np.random.randn(100_000).astype(np.float32)
B = np.random.randn(100_000).astype(np.float32)

result = device_array_empty(100_000, dtype = np.float32)

foo_kernel[use_gpu, threads_per_block, result.shape[0]](result, A, B)

print(result.copy_to_host())
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class jit(object):

    &#34;&#34;&#34;
    Decorator to recompile Python functions into native CPU/GPU ones.
    &#34;&#34;&#34;

    ####################################################################################################################

    @staticmethod
    def grid(ndim: int) -&gt; None:

        &#34;&#34;&#34;
        Return the absolute position of the current thread in the entire grid of blocks.

        Parameters
        ----------
        ndim : int
            Number of dimensions.
        &#34;&#34;&#34;

        pass

    ####################################################################################################################

    @staticmethod
    def local_empty(shape: Union[tuple, int], dtype: Type[np.single] = np.float32) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Allocate an empty device ndarray in the local memory. Similar to `np.empty()`.

        Parameters
        ----------
        shape : Union[tuple, int]
            Desired shape for the new array.
        dtype : Type[np.single]
            Desired data-type for the new array.
        &#34;&#34;&#34;

        pass

    ####################################################################################################################

    @staticmethod
    def shared_empty(shape: Union[tuple, int], dtype: Type[np.single] = np.float32) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Allocate an empty device ndarray in the shared memory. Similar to `np.empty()`.

        Parameters
        ----------
        shape : Union[tuple, int]
            Desired shape for the new array.
        dtype : Type[np.single]
            Desired data-type for the new array.
        &#34;&#34;&#34;

        pass

    ####################################################################################################################

    @staticmethod
    def syncthreads() -&gt; None:

        &#34;&#34;&#34;
        Synchronize all threads in the same thread block.
        &#34;&#34;&#34;

        pass

    ####################################################################################################################

    @staticmethod
    def atomic_add(array: np.ndarray, idx: int, val: Union[np.int32, np.float32, np.float64]) -&gt; Union[np.int32, np.float32, np.float64]:

        &#34;&#34;&#34;
        Performs atomic array[idx] += val and returns the old value. Supported on int32, float32, and float64 operands only.

        Parameters
        ----------
        array : np.ndarray
            Array to be modified.
        idx : int
            Index in the array.
        val : Union[np.int32, np.float32, np.float64]
            New value.
        &#34;&#34;&#34;

        pass

    ####################################################################################################################

    @staticmethod
    def atomic_sub(array: np.ndarray, idx: int, val: Union[np.int32, np.float32, np.float64]) -&gt; Union[np.int32, np.float32, np.float64]:

        &#34;&#34;&#34;
        Performs atomic array[idx] -= val and returns the old value. Supported on int32, float32, and float64 operands only.

        Parameters
        ----------
        array : np.ndarray
            Array to be modified.
        idx : int
            Index in the array.
        val : Union[np.int32, np.float32, np.float64]
            New value.
        &#34;&#34;&#34;

        pass

    ####################################################################################################################

    _CALL_RE = re.compile(&#39;(\\w+)_xpu\\s*\\(&#39;)

    _METHOD_RE = re.compile(&#39;def[^(]+(\\(.*)&#39;, flags = re.DOTALL)

    _CPU_CODE_RE = re.compile(re.escape(&#39;!--BEGIN-CPU--&#39;) + &#39;.*?&#39; + re.escape(&#39;!--END-CPU--&#39;), re.DOTALL)

    _GPU_CODE_RE = re.compile(re.escape(&#39;!--BEGIN-GPU--&#39;) + &#39;.*?&#39; + re.escape(&#39;!--END-GPU--&#39;), re.DOTALL)

    ####################################################################################################################

    def __init__(self, kernel: bool = False, fastmath: bool = False, parallel: bool = False):

        &#34;&#34;&#34;
        Parameters
        ----------
        kernel : bool
            Indicates whether this function is a CPU/GPU kernel (default: **False**).
        fastmath : bool
            Enables fast-math optimizations when running on CPU (default: **False**).
        parallel : bool
            Enables automatic parallelization when running on CPU (default: **False**).

        Example
        -------
            @jit(parallel = False)
            def foo_xpu(a, b):

                return a + b

            @jit(kernel = True)
            def foo_kernel(result, a, b):

                ########################################################################
                # !--BEGIN-CPU--

                for i in range(result.shape[0]):

                    result[i] = foo_xpu(a[i], b[i])

                # !--END-CPU--
                ########################################################################
                # !--BEGIN-GPU--

                i = jit.grid(1)
                if i &lt; result.shape[0]:

                    result[i] = foo_xpu(a[i], b[i])

                # !--END-GPU--
                ########################################################################

            use_gpu = True
            threads_per_block = 32

            A = np.random.randn(100_000).astype(np.float32)
            B = np.random.randn(100_000).astype(np.float32)

            result = device_array_empty(100_000, dtype = np.float32)

            foo_kernel[use_gpu, threads_per_block, result.shape[0]](result, A, B)

            print(result.copy_to_host())
        &#34;&#34;&#34;

        self._kernel = kernel
        self._fastmath = fastmath
        self._parallel = parallel

    ####################################################################################################################

    _cnt = 0

    @classmethod
    def _get_unique_function_name(cls) -&gt; str:

        name = f&#39;__jit_f{cls._cnt}&#39;

        cls._cnt = cls._cnt + 1

        return name

    ####################################################################################################################

    @classmethod
    def _patch_cpu_code(cls, code: str) -&gt; str:

        code_cpu = cls._CALL_RE.sub(lambda m: f&#39;jit_module.{m.group(1)}_cpu(&#39;, cls._GPU_CODE_RE.sub(&#39;&#39;, code))

        return (
            code_cpu
            .replace(&#39;jit.grid&#39;, &#39;# jit.grid&#39;)
            .replace(&#39;jit.local_empty&#39;, &#39;np.empty&#39;)
            .replace(&#39;jit.shared_empty&#39;, &#39;np.empty&#39;)
            .replace(&#39;jit.syncthreads&#39;, &#39;# jit.syncthreads&#39;)
            .replace(&#39;jit.atomic_add&#39;, &#39;jit_module.atomic.add&#39;)
            .replace(&#39;jit.atomic_sub&#39;, &#39;jit_module.atomic.sub&#39;)
        )

    ####################################################################################################################

    @classmethod
    def _patch_gpu_code(cls, code: str) -&gt; str:

        code_gpu = cls._CALL_RE.sub(lambda m: f&#39;jit_module.{m.group(1)}_gpu(&#39;, cls._CPU_CODE_RE.sub(&#39;&#39;, code))

        return (
            code_gpu
            .replace(&#39;jit.grid&#39;, &#39;cuda_module.grid&#39;)
            .replace(&#39;jit.local_empty&#39;, &#39;cuda_module.local.array&#39;)
            .replace(&#39;jit.shared_empty&#39;, &#39;cuda_module.shared.array&#39;)
            .replace(&#39;jit.syncthreads&#39;, &#39;cuda_module.syncthreads&#39;)
            .replace(&#39;jit.atomic_add&#39;, &#39;cuda_module.atomic.add&#39;)
            .replace(&#39;jit.atomic_sub&#39;, &#39;cuda_module.atomic.sub&#39;)
        )

    ####################################################################################################################

    @classmethod
    def _inject_cpu_funct(cls, orig_funct: Callable, new_funct: Callable) -&gt; None:

        globals()[orig_funct.__name__.replace(&#39;_xpu&#39;, &#39;_cpu&#39;)] = new_funct

    ####################################################################################################################

    @classmethod
    def _inject_gpu_funct(cls, orig_funct: Callable, new_funct: Callable) -&gt; None:

        globals()[orig_funct.__name__.replace(&#39;_xpu&#39;, &#39;_gpu&#39;)] = new_funct

    ####################################################################################################################

    def __call__(self, funct: Callable):

        if not self._kernel and not funct.__name__.endswith(&#39;_xpu&#39;):

            raise Exception(f&#39;Function `{funct.__name__}` name must ends with `_xpu`&#39;)

        ################################################################################################################
        # FRAME                                                                                                        #
        ################################################################################################################

        me = sys.modules[__name__]

        funct.__globals__[&#39;jit_module&#39;] = me

        funct.__globals__[&#39;cuda_module&#39;] = cu

        ################################################################################################################
        # SOURCE CODE                                                                                                  #
        ################################################################################################################

        code_raw = jit._METHOD_RE.search(inspect.getsource(funct)).group(1)

        ################################################################################################################
        # NUMBA ON GPU                                                                                                 #
        ################################################################################################################

        name_cpu = jit._get_unique_function_name()

        code_cpu = jit._patch_cpu_code(f&#39;def {name_cpu} {code_raw}&#39;)

        ################################################################################################################

        exec(code_cpu, funct.__globals__)

        funct_cpu = eval(name_cpu, funct.__globals__)

        if not self._kernel:

            jit._inject_cpu_funct(funct, nb.njit(funct_cpu, fastmath = self._fastmath, parallel = self._parallel) if CPU_OPTIMIZATION_AVAILABLE else funct_cpu)

        ################################################################################################################
        # NUMBA ON CPU                                                                                                 #
        ################################################################################################################

        name_gpu = jit._get_unique_function_name()

        code_gpu = jit._patch_gpu_code(f&#39;def {name_gpu} {code_raw}&#39;)

        ################################################################################################################

        exec(code_gpu, funct.__globals__)

        funct_gpu = eval(name_gpu, funct.__globals__)

        if not self._kernel:

            jit._inject_gpu_funct(funct, cu.jit(funct_gpu, device = True) if GPU_OPTIMIZATION_AVAILABLE else dont_call)

        ################################################################################################################
        # KERNEL                                                                                                       #
        ################################################################################################################

        funct = Kernel(funct_cpu, funct_gpu, fastmath = self._fastmath, parallel = self._parallel) if self._kernel else dont_call

        ################################################################################################################

        return funct</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="decontamination.jit.grid"><code class="name flex">
<span>def <span class="ident">grid</span></span>(<span>ndim:Â int) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Return the absolute position of the current thread in the entire grid of blocks.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ndim</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of dimensions.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def grid(ndim: int) -&gt; None:

    &#34;&#34;&#34;
    Return the absolute position of the current thread in the entire grid of blocks.

    Parameters
    ----------
    ndim : int
        Number of dimensions.
    &#34;&#34;&#34;

    pass</code></pre>
</details>
</dd>
<dt id="decontamination.jit.local_empty"><code class="name flex">
<span>def <span class="ident">local_empty</span></span>(<span>shape:Â Union[tuple,Â int], dtype:Â Type[np.float32]Â =Â np.float32) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Allocate an empty device ndarray in the local memory. Similar to <code>np.empty()</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>Union[tuple, int]</code></dt>
<dd>Desired shape for the new array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Desired data-type for the new array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def local_empty(shape: Union[tuple, int], dtype: Type[np.single] = np.float32) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Allocate an empty device ndarray in the local memory. Similar to `np.empty()`.

    Parameters
    ----------
    shape : Union[tuple, int]
        Desired shape for the new array.
    dtype : Type[np.single]
        Desired data-type for the new array.
    &#34;&#34;&#34;

    pass</code></pre>
</details>
</dd>
<dt id="decontamination.jit.shared_empty"><code class="name flex">
<span>def <span class="ident">shared_empty</span></span>(<span>shape:Â Union[tuple,Â int], dtype:Â Type[np.float32]Â =Â np.float32) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Allocate an empty device ndarray in the shared memory. Similar to <code>np.empty()</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>Union[tuple, int]</code></dt>
<dd>Desired shape for the new array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Desired data-type for the new array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def shared_empty(shape: Union[tuple, int], dtype: Type[np.single] = np.float32) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Allocate an empty device ndarray in the shared memory. Similar to `np.empty()`.

    Parameters
    ----------
    shape : Union[tuple, int]
        Desired shape for the new array.
    dtype : Type[np.single]
        Desired data-type for the new array.
    &#34;&#34;&#34;

    pass</code></pre>
</details>
</dd>
<dt id="decontamination.jit.syncthreads"><code class="name flex">
<span>def <span class="ident">syncthreads</span></span>(<span>) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Synchronize all threads in the same thread block.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def syncthreads() -&gt; None:

    &#34;&#34;&#34;
    Synchronize all threads in the same thread block.
    &#34;&#34;&#34;

    pass</code></pre>
</details>
</dd>
<dt id="decontamination.jit.atomic_add"><code class="name flex">
<span>def <span class="ident">atomic_add</span></span>(<span>array:Â np.ndarray, idx:Â int, val:Â Union[np.int32,Â np.float32,Â np.float64]) â€‘>Â Union[np.int32,Â np.float32,Â np.float64]</span>
</code></dt>
<dd>
<div class="desc"><p>Performs atomic array[idx] += val and returns the old value. Supported on int32, float32, and float64 operands only.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>array</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array to be modified.</dd>
<dt><strong><code>idx</code></strong> :&ensp;<code>int</code></dt>
<dd>Index in the array.</dd>
<dt><strong><code>val</code></strong> :&ensp;<code>Union[np.int32, np.float32, np.float64]</code></dt>
<dd>New value.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def atomic_add(array: np.ndarray, idx: int, val: Union[np.int32, np.float32, np.float64]) -&gt; Union[np.int32, np.float32, np.float64]:

    &#34;&#34;&#34;
    Performs atomic array[idx] += val and returns the old value. Supported on int32, float32, and float64 operands only.

    Parameters
    ----------
    array : np.ndarray
        Array to be modified.
    idx : int
        Index in the array.
    val : Union[np.int32, np.float32, np.float64]
        New value.
    &#34;&#34;&#34;

    pass</code></pre>
</details>
</dd>
<dt id="decontamination.jit.atomic_sub"><code class="name flex">
<span>def <span class="ident">atomic_sub</span></span>(<span>array:Â np.ndarray, idx:Â int, val:Â Union[np.int32,Â np.float32,Â np.float64]) â€‘>Â Union[np.int32,Â np.float32,Â np.float64]</span>
</code></dt>
<dd>
<div class="desc"><p>Performs atomic array[idx] -= val and returns the old value. Supported on int32, float32, and float64 operands only.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>array</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array to be modified.</dd>
<dt><strong><code>idx</code></strong> :&ensp;<code>int</code></dt>
<dd>Index in the array.</dd>
<dt><strong><code>val</code></strong> :&ensp;<code>Union[np.int32, np.float32, np.float64]</code></dt>
<dd>New value.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def atomic_sub(array: np.ndarray, idx: int, val: Union[np.int32, np.float32, np.float64]) -&gt; Union[np.int32, np.float32, np.float64]:

    &#34;&#34;&#34;
    Performs atomic array[idx] -= val and returns the old value. Supported on int32, float32, and float64 operands only.

    Parameters
    ----------
    array : np.ndarray
        Array to be modified.
    idx : int
        Index in the array.
    val : Union[np.int32, np.float32, np.float64]
        New value.
    &#34;&#34;&#34;

    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="decontamination.DeviceArray"><code class="flex name class">
<span>class <span class="ident">DeviceArray</span></span>
<span>(</span><span>shape:Â Union[tuple,Â int], dtype:Â Type[np.float32]Â =Â np.float32, content:Â Union[int,Â float,Â np.ndarray,Â ForwardRef(None)]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Device array to be used when calling a CPU/GPU kernel.</p>
<p>Prefer using primitives <code><a title="decontamination.device_array_from" href="#decontamination.device_array_from">device_array_from()</a></code>, <code><a title="decontamination.device_array_empty" href="#decontamination.device_array_empty">device_array_empty()</a></code>, <code><a title="decontamination.device_array_zeros" href="#decontamination.device_array_zeros">device_array_zeros()</a></code>, <code><a title="decontamination.device_array_full" href="#decontamination.device_array_full">device_array_full()</a></code>
to instantiate a device array.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>Union[tuple, int]</code></dt>
<dd>Desired shape for the new array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Desired data-type for the new array.</dd>
<dt><strong><code>content</code></strong> :&ensp;<code>Optional[Union[int, float, np.ndarray]]</code></dt>
<dd>Optional content, integer, floating ot Numpy ndarray.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeviceArray(object):

    &#34;&#34;&#34;
    Device array to be used when calling a CPU/GPU kernel.

    Prefer using primitives `device_array_from`, `device_array_empty`, `device_array_zeros`, `device_array_full`
    to instantiate a device array.
    &#34;&#34;&#34;

    ####################################################################################################################

    def __init__(self, shape: Union[tuple, int], dtype: Type[np.single] = np.float32, content: Optional[Union[int, float, np.ndarray]] = None):

        &#34;&#34;&#34;
        Parameters
        ----------
        shape : Union[tuple, int]
            Desired shape for the new array.
        dtype : Type[np.single]
            Desired data-type for the new array.
        content : Optional[Union[int, float, np.ndarray]]
            Optional content, integer, floating ot Numpy ndarray.
        &#34;&#34;&#34;

        self._shape = shape
        self._dtype = dtype

        self._content = content

        self._instance = None

    ####################################################################################################################

    @property
    def shape(self):

        &#34;&#34;&#34;
        Shape of the array.
        &#34;&#34;&#34;

        return self._shape

    ####################################################################################################################

    @property
    def dtype(self):

        &#34;&#34;&#34;
        Data-type of the array.
        &#34;&#34;&#34;

        return self._dtype

    ####################################################################################################################

    def _instantiate(self, is_gpu: bool):

        ################################################################################################################

        if self._instance is not None:

            return self._instance

        ################################################################################################################

        if is_gpu:

            ############################################################################################################
            # GPU INSTANTIATION                                                                                        #
            ############################################################################################################

            if isinstance(self._content, np.ndarray):

                self._instance = cu.to_device(self._content)

            elif self._content is not None:

                if float(self._content) == 0.0:
                    self._instance = cu.to_device(np.zeros(self._shape, dtype = self._dtype))
                else:
                    self._instance = cu.to_device(np.full(self._shape, self._content, dtype = self._dtype))

            else:

                self._instance = cu.device_array(self._shape, dtype = self._dtype)

            ############################################################################################################

        else:

            ############################################################################################################
            # CPU INSTANTIATION                                                                                        #
            ############################################################################################################

            if isinstance(self._content, np.ndarray):

                self._instance = nb_to_device(self._content)

            elif self._content is not None:

                if float(self._content) == 0.0:
                    self._instance = np.zeros(self._shape, dtype = self._dtype)
                else:
                    self._instance = np.full(self._shape, self._content, dtype = self._dtype)

            else:

                self._instance = np.empty(self._shape, dtype = self._dtype)

        ################################################################################################################

        return self._instance

    ####################################################################################################################

    def copy_to_host(self) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Create a new Numpy ndarray from the underlying device ndarray.
        &#34;&#34;&#34;

        ################################################################################################################

        if self._instance is None:

            raise Exception(&#39;Device array not instanced&#39;)

        ################################################################################################################

        if cu.is_cuda_array(self._instance):

            return self._instance.copy_to_host()

        else:

            return self._instance</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="decontamination.DeviceArray.shape"><code class="name">var <span class="ident">shape</span></code></dt>
<dd>
<div class="desc"><p>Shape of the array.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self):

    &#34;&#34;&#34;
    Shape of the array.
    &#34;&#34;&#34;

    return self._shape</code></pre>
</details>
</dd>
<dt id="decontamination.DeviceArray.dtype"><code class="name">var <span class="ident">dtype</span></code></dt>
<dd>
<div class="desc"><p>Data-type of the array.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dtype(self):

    &#34;&#34;&#34;
    Data-type of the array.
    &#34;&#34;&#34;

    return self._dtype</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="decontamination.DeviceArray.copy_to_host"><code class="name flex">
<span>def <span class="ident">copy_to_host</span></span>(<span>self) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Create a new Numpy ndarray from the underlying device ndarray.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy_to_host(self) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Create a new Numpy ndarray from the underlying device ndarray.
    &#34;&#34;&#34;

    ################################################################################################################

    if self._instance is None:

        raise Exception(&#39;Device array not instanced&#39;)

    ################################################################################################################

    if cu.is_cuda_array(self._instance):

        return self._instance.copy_to_host()

    else:

        return self._instance</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="decontamination.SOM_Abstract"><code class="flex name class">
<span>class <span class="ident">SOM_Abstract</span></span>
<span>(</span><span>m:Â int, n:Â int, dim:Â int, dtype:Â Type[np.float32]Â =Â np.float32, topology:Â Optional[str]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Self Organizing Maps (abstract class).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron rows.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron columns.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of the input data.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Neural network data type (default: <strong>np.float32</strong>).</dd>
<dt><strong><code>topology</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Topology of the model, either <strong>'square'</strong> or <strong>'hexagonal'</strong> (default: <strong>None</strong>, uses: <strong>'hexagonal'</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SOM_Abstract(object):

    &#34;&#34;&#34;
    Self Organizing Maps (abstract class).
    &#34;&#34;&#34;

    __MODE__ = &#39;abstract&#39;

    ####################################################################################################################

    def __init__(self, m: int, n: int, dim: int, dtype: Type[np.single] = np.float32, topology: Optional[str] = None):

        &#34;&#34;&#34;
        Parameters
        ----------
        m : int
            Number of neuron rows.
        n : int
            Number of neuron columns.
        dim : int
            Dimensionality of the input data.
        dtype : Type[np.single]
            Neural network data type (default: **np.float32**).
        topology : Optional[str]
            Topology of the model, either **&#39;square&#39;** or **&#39;hexagonal&#39;** (default: **None**, uses: **&#39;hexagonal&#39;**).
        &#34;&#34;&#34;

        ################################################################################################################

        self._m = m
        self._n = n
        self._dim = dim
        self._dtype = dtype
        self._topology = topology or &#39;hexagonal&#39;

        ################################################################################################################

        self._rebuild_topography()

        ################################################################################################################

        self._weights = np.empty((self._m * self._n, self._dim), dtype = self._dtype)

        self._quantization_errors = np.empty(0, dtype = np.float32)

        self._topographic_errors = np.empty(0, dtype = np.float32)

        ################################################################################################################

        self._header_extra = {
            &#39;mode&#39;: &#39;__MODE__&#39;,
        }

        self._dataset_extra = {
        }

    ####################################################################################################################

    @staticmethod
    def _neuron_locations_square(m: int, n: int) -&gt; Iterator[List[int]]:

        for i in range(m):
            for j in range(n):

                yield [i, j]

    ####################################################################################################################

    @staticmethod
    def _neuron_locations_hexagonal(m: int, n: int) -&gt; Iterator[List[float]]:

        for i in range(m):
            for j in range(n):

                i_offset = (j &amp; 1) * (-0.5)

                yield [i + i_offset, j * 0.8660254037844386]  # âˆš3/2 = 0.8660254037844386

    ####################################################################################################################

    def _rebuild_topography(self):

        if self._topology == &#39;square&#39;:

            self._topography = np.array(list(SOM_Abstract._neuron_locations_square(self._m, self._n)), dtype = np.float32)

        else:

            self._topography = np.array(list(SOM_Abstract._neuron_locations_hexagonal(self._m, self._n)), dtype = np.float32)

    ####################################################################################################################

    def init_rand(self, seed: Optional[int] = None) -&gt; None:

        &#34;&#34;&#34;
        Initializes the neural network randomly.

        Parameters
        ----------
        seed : Optional[int]
            Seed for random generator (default: **None**).
        &#34;&#34;&#34;

        ################################################################################################################

        if seed is None:

            rng = np.random.default_rng()

        else:

            rng = np.random.default_rng(seed = seed)

        ################################################################################################################

        self._weights[...] = rng.random((self._m * self._n, self._dim), dtype = self._dtype)

    ####################################################################################################################

    def init_from(self, other: &#39;SOM_Abstract&#39;) -&gt; None:

        &#34;&#34;&#34;
        Initializes the neural network from another one.

        Parameters
        ----------
        other : SOM_Abstract
            Another SOM object from which the weights will be copied.
        &#34;&#34;&#34;

        if self._m != other._m              \
           or                               \
           self._n != other._n              \
           or                               \
           self._dim != other._dim          \
           or                               \
           self._dtype != other._dtype      \
           or                               \
           self._topology != other._topology:

            raise Exception(&#39;Incompatible shapes, dtypes or topologies&#39;)

        self._weights[...] = other._weights

    ####################################################################################################################

    def _init_hdf5_extra(self):

        ################################################################################################################

        header_extra = {name: field for name, field in self._header_extra.items()}

        dataset_extra = {name: field for name, field in self._dataset_extra.items()}

        ################################################################################################################

        header_extra[&#39;m&#39;] = &#39;_m&#39;
        header_extra[&#39;n&#39;] = &#39;_n&#39;
        header_extra[&#39;dim&#39;] = &#39;_dim&#39;
        header_extra[&#39;topology&#39;] = &#39;_topology&#39;

        dataset_extra[&#39;weights&#39;] = &#39;_weights&#39;
        dataset_extra[&#39;quantization_errors&#39;] = &#39;_quantization_errors&#39;
        dataset_extra[&#39;topographic_errors&#39;] = &#39;_topographic_errors&#39;

        ################################################################################################################

        return header_extra, dataset_extra

    ####################################################################################################################

    def save(self, filename: str) -&gt; None:

        &#34;&#34;&#34;
        Saves the trained neural network to a HDF5 file.

        Parameters
        ----------
        filename : str
            Output filename.
        &#34;&#34;&#34;

        ################################################################################################################

        import h5py

        ################################################################################################################

        header_extra, dataset_extra = self._init_hdf5_extra()

        ################################################################################################################

        with h5py.File(filename, mode = &#39;w&#39;) as file:

            model_group = file.create_group(&#39;model&#39;, track_order = True)

            ############################################################################################################
            # HEADERS                                                                                                  #
            ############################################################################################################

            for name, field in header_extra.items():

                data = getattr(self, field)

                if data is not None:

                    model_group.attrs[name] = data

            ############################################################################################################
            # DATASETS                                                                                                 #
            ############################################################################################################

            for name, field in dataset_extra.items():

                data = getattr(self, field)

                if data is not None:

                    model_group.create_dataset(
                        name,
                        data = data,
                        shape = data.shape,
                        dtype = data.dtype
                    )

        ################################################################################################################

        self._rebuild_topography()

    ####################################################################################################################

    def load(self, filename: str) -&gt; None:

        &#34;&#34;&#34;
        Loads the trained neural network from a HDF5 file.

        Parameters
        ----------
        filename : str
            Input filename.
        &#34;&#34;&#34;

        ################################################################################################################

        import h5py

        ################################################################################################################

        header_extra, dataset_extra = self._init_hdf5_extra()

        ################################################################################################################

        with h5py.File(filename, mode = &#39;r&#39;) as file:

            model_group = file[&#39;model&#39;]

            ############################################################################################################
            # HEADERS                                                                                                  #
            ############################################################################################################

            for name, field in header_extra.items():

                try:

                    value = model_group.attrs[name]

                    if isinstance(value, np.int32) or isinstance(value, np.int64):

                        setattr(self, field, int(value))

                    elif isinstance(value, np.float32) or isinstance(value, np.float64):

                        setattr(self, field, float(value))

                    else:

                        setattr(self, field, value)

                except KeyError:

                    pass

            ############################################################################################################
            # DATASETS                                                                                                 #
            ############################################################################################################

            for name, field in dataset_extra.items():

                try:

                    array = np.array(model_group[name])

                    array = array.astype(self._dtype)

                    setattr(self, field, array)

                except KeyError:

                    pass

        ################################################################################################################

        self._rebuild_topography()

    ####################################################################################################################

    @property
    def m(self) -&gt; int:

        &#34;&#34;&#34;
        Returns the number of neuron rows.
        &#34;&#34;&#34;

        return self._m

    ####################################################################################################################

    @property
    def n(self) -&gt; int:

        &#34;&#34;&#34;
        Returns the number of neuron columns.
        &#34;&#34;&#34;

        return self._n

    ####################################################################################################################

    @property
    def dim(self) -&gt; int:

        &#34;&#34;&#34;
        Returns the dimensionality of the input data.
        &#34;&#34;&#34;

        return self._dim

    ####################################################################################################################

    @property
    def dtype(self) -&gt; Type[np.single]:

        &#34;&#34;&#34;
        Returns the neural network data type.
        &#34;&#34;&#34;

        return self._dtype

    ####################################################################################################################

    @property
    def topology(self) -&gt; str:

        &#34;&#34;&#34;
        Returns the model topology, either **&#39;square&#39;** or **&#39;hexagonal&#39;**.
        &#34;&#34;&#34;

        return self._topology

    ####################################################################################################################

    def get_weights(self) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns the neural network weights with the shape [m * n, dim].
        &#34;&#34;&#34;

        return self._weights.reshape((self._m * self._n, self._dim))

    ####################################################################################################################

    def get_centroids(self) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns the neural network weights with the shape [m, n, dim].
        &#34;&#34;&#34;

        return self._weights.reshape((self._m, self._n, self._dim))

    ####################################################################################################################

    def get_quantization_errors(self) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns the quantization error. $$ c_i^1\\equiv\\mathrm{1^\\mathrm{st}\\,bmu}\\equiv\\underset{j}{\\mathrm{arg\\,min}_1}\\lVert x_i-w_j\\rVert $$ $$ \\boxed{e_Q\\equiv\\frac{1}{N}\\sum_{i=1}^N\\lVert x_i-w_{c_i^1}\\rVert} $$
        &#34;&#34;&#34;

        return self._quantization_errors

    ####################################################################################################################

    def get_topographic_errors(self) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns the topographic errors. $$ c_i^n\\equiv\\mathrm{n^\\mathrm{th}\\,bmu}\\equiv\\underset{j}{\\mathrm{arg\\,min}_n}\\lVert x_i-w_j\\rVert $$ $$ r\\equiv\\left\\{\\begin{array}{ll}\\sqrt{1}&amp;\\mathrm{topology=hexagon}\\\\\\sqrt{2}&amp;\\mathrm{topology=square}\\end{array}\\right. $$ $$ t(x_i)\\equiv\\left\\{\\begin{array}{ll}1&amp;\\lVert c_i^1-c_i^2\\rVert&gt;r\\\\0&amp;\\mathrm{otherwise}\\end{array}\\right. $$ $$ \\boxed{e_t\\equiv\\frac{1}{N}\\sum_{i=0}^Nt(x_i)} $$
        &#34;&#34;&#34;

        return self._topographic_errors

    ####################################################################################################################

    _X_HEX_STENCIL = np.array([
        +1, +1, +1, +0, -1, +0,  # Even line
        +0, +1, +0, -1, -1, -1,  # Odd line
    ], dtype = int)

    _Y_HEX_STENCIL = np.array([
        +1, +0, -1, -1, +0, +1,  # Even line
        +1, +0, -1, -1, +0, +1,  # Odd line
    ], dtype = int)

    ####################################################################################################################

    _X_SQU_STENCIL = np.array([
        +0, -1, -1, -1, +0, +1, +1, +1,  # Even line
        +0, -1, -1, -1, +0, +1, +1, +1,  # Odd line
    ], dtype = int)

    _Y_SQU_STENCIL = np.array([
        -1, -1, +0, +1, +1, +1, +0, -1,  # Even line
        -1, -1, +0, +1, +1, +1, +0, -1,  # Odd line
    ], dtype = int)

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _distance_map(result, centroids: np.ndarray, x_stencil: np.ndarray, y_stencil: np.ndarray, m: int, n: int, l: int) -&gt; None:

        for x in range(m):
            for y in range(n):

                offset = (y &amp; 1) * l

                w1 = centroids[x, y]

                for z in range(l):

                    i = x + x_stencil[z + offset]
                    j = y + y_stencil[z + offset]

                    if 0 &lt;= i &lt; m and 0 &lt;= j &lt; n:

                        w2 = centroids[i, j]

                        result[x, y, z] = np.sqrt(np.sum((w1 - w2) ** 2))

    ####################################################################################################################

    def get_distance_map(self, scaling: Optional[str] = None) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns a matrix of the distances between each weight.

        Parameters
        ----------
        scaling : Optional[str]
            Normalization method, either &#39;**sum**&#39; or &#39;**mean**&#39; (default: **None**, uses: &#39;**sum**&#39;).
        &#34;&#34;&#34;

        scaling = scaling or &#39;sum&#39;

        ################################################################################################################

        if self._topology == &#39;square&#39;:

            result = np.full((self._m, self._n, 8), np.nan, dtype = self._dtype)

            SOM_Abstract._distance_map(result, self.get_centroids(), SOM_Abstract._X_SQU_STENCIL, SOM_Abstract._Y_SQU_STENCIL, self._m, self._n, 8)

        else:

            result = np.full((self._m, self._n, 6), np.nan, dtype = self._dtype)

            SOM_Abstract._distance_map(result, self.get_centroids(), SOM_Abstract._X_HEX_STENCIL, SOM_Abstract._Y_HEX_STENCIL, self._m, self._n, 6)

        ################################################################################################################

        if scaling == &#39;sum&#39;:
            result = np.nansum(result, axis = 2)

        elif scaling == &#39;mean&#39;:
            result = np.nanmean(result, axis = 2)

        else:
            raise Exception(f&#39;Invalid scaling method `{scaling}`&#39;)

        ################################################################################################################

        return result / np.max(result)

    ####################################################################################################################

    def get_activation_map(self, dataset: Union[np.ndarray, Callable], enable_gpu: bool = True, threads_per_blocks: int = 1024) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns a matrix containing the number of times each neuron have been activated for the given input.

        Parameters
        ----------
        dataset : Union[np.ndarray, Callable]
            Dataset array or generator builder.
        enable_gpu : bool
            If available, run on GPU rather than CPU (default: **True**).
        threads_per_blocks : int
            Number of GPU threads per blocks (default: **1024**).
        &#34;&#34;&#34;

        ################################################################################################################

        generator_builder = dataset_to_generator_builder(dataset)

        ################################################################################################################

        result = device_array_zeros(shape = (self._m * self._n, ), dtype = np.int64)

        ################################################################################################################

        generator = generator_builder()

        for data in generator():

            _count_bmus_kernel[enable_gpu, threads_per_blocks, data.shape[0]](result, self._weights, data, self._m * self._n)

        ################################################################################################################

        return result.copy_to_host().reshape((self._m, self._n))

    ####################################################################################################################

    def get_winners(self, dataset: np.ndarray, enable_gpu: bool = True, threads_per_blocks: int = 1024) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Returns a vector of the best matching unit indices (shape mÃ—n) for the given input.

        Parameters
        ----------
        dataset : np.ndarray
            Dataset array.
        enable_gpu : bool
            If available, run on GPU rather than CPU (default: **True**).
        threads_per_blocks : int
            Number of GPU threads per blocks (default: **1024**).
        &#34;&#34;&#34;

        ################################################################################################################

        result = device_array_empty(dataset.shape[0], dtype = np.int32)

        ################################################################################################################

        _find_bmus_kernel[enable_gpu, threads_per_blocks, dataset.shape[0]](result, self._weights, dataset, self._m * self._n)

        ################################################################################################################

        return result.copy_to_host()</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li>decontamination._algo.som_batch.SOM_Batch</li>
<li>decontamination._algo.som_online.SOM_Online</li>
<li>decontamination._algo.som_pca.SOM_PCA</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="decontamination.SOM_Abstract.m"><code class="name">var <span class="ident">m</span> :Â int</code></dt>
<dd>
<div class="desc"><p>Returns the number of neuron rows.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def m(self) -&gt; int:

    &#34;&#34;&#34;
    Returns the number of neuron rows.
    &#34;&#34;&#34;

    return self._m</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.n"><code class="name">var <span class="ident">n</span> :Â int</code></dt>
<dd>
<div class="desc"><p>Returns the number of neuron columns.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def n(self) -&gt; int:

    &#34;&#34;&#34;
    Returns the number of neuron columns.
    &#34;&#34;&#34;

    return self._n</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.dim"><code class="name">var <span class="ident">dim</span> :Â int</code></dt>
<dd>
<div class="desc"><p>Returns the dimensionality of the input data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dim(self) -&gt; int:

    &#34;&#34;&#34;
    Returns the dimensionality of the input data.
    &#34;&#34;&#34;

    return self._dim</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.dtype"><code class="name">var <span class="ident">dtype</span> :Â Type[np.float32]</code></dt>
<dd>
<div class="desc"><p>Returns the neural network data type.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dtype(self) -&gt; Type[np.single]:

    &#34;&#34;&#34;
    Returns the neural network data type.
    &#34;&#34;&#34;

    return self._dtype</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.topology"><code class="name">var <span class="ident">topology</span> :Â str</code></dt>
<dd>
<div class="desc"><p>Returns the model topology, either <strong>'square'</strong> or <strong>'hexagonal'</strong>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def topology(self) -&gt; str:

    &#34;&#34;&#34;
    Returns the model topology, either **&#39;square&#39;** or **&#39;hexagonal&#39;**.
    &#34;&#34;&#34;

    return self._topology</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="decontamination.SOM_Abstract.init_rand"><code class="name flex">
<span>def <span class="ident">init_rand</span></span>(<span>self, seed:Â Optional[int]Â =Â None) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes the neural network randomly.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>seed</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Seed for random generator (default: <strong>None</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_rand(self, seed: Optional[int] = None) -&gt; None:

    &#34;&#34;&#34;
    Initializes the neural network randomly.

    Parameters
    ----------
    seed : Optional[int]
        Seed for random generator (default: **None**).
    &#34;&#34;&#34;

    ################################################################################################################

    if seed is None:

        rng = np.random.default_rng()

    else:

        rng = np.random.default_rng(seed = seed)

    ################################################################################################################

    self._weights[...] = rng.random((self._m * self._n, self._dim), dtype = self._dtype)</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.init_from"><code class="name flex">
<span>def <span class="ident">init_from</span></span>(<span>self, other:Â <a title="decontamination.SOM_Abstract" href="#decontamination.SOM_Abstract">SOM_Abstract</a>) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes the neural network from another one.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>other</code></strong> :&ensp;<code><a title="decontamination.SOM_Abstract" href="#decontamination.SOM_Abstract">SOM_Abstract</a></code></dt>
<dd>Another SOM object from which the weights will be copied.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_from(self, other: &#39;SOM_Abstract&#39;) -&gt; None:

    &#34;&#34;&#34;
    Initializes the neural network from another one.

    Parameters
    ----------
    other : SOM_Abstract
        Another SOM object from which the weights will be copied.
    &#34;&#34;&#34;

    if self._m != other._m              \
       or                               \
       self._n != other._n              \
       or                               \
       self._dim != other._dim          \
       or                               \
       self._dtype != other._dtype      \
       or                               \
       self._topology != other._topology:

        raise Exception(&#39;Incompatible shapes, dtypes or topologies&#39;)

    self._weights[...] = other._weights</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, filename:Â str) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the trained neural network to a HDF5 file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Output filename.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, filename: str) -&gt; None:

    &#34;&#34;&#34;
    Saves the trained neural network to a HDF5 file.

    Parameters
    ----------
    filename : str
        Output filename.
    &#34;&#34;&#34;

    ################################################################################################################

    import h5py

    ################################################################################################################

    header_extra, dataset_extra = self._init_hdf5_extra()

    ################################################################################################################

    with h5py.File(filename, mode = &#39;w&#39;) as file:

        model_group = file.create_group(&#39;model&#39;, track_order = True)

        ############################################################################################################
        # HEADERS                                                                                                  #
        ############################################################################################################

        for name, field in header_extra.items():

            data = getattr(self, field)

            if data is not None:

                model_group.attrs[name] = data

        ############################################################################################################
        # DATASETS                                                                                                 #
        ############################################################################################################

        for name, field in dataset_extra.items():

            data = getattr(self, field)

            if data is not None:

                model_group.create_dataset(
                    name,
                    data = data,
                    shape = data.shape,
                    dtype = data.dtype
                )

    ################################################################################################################

    self._rebuild_topography()</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, filename:Â str) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the trained neural network from a HDF5 file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Input filename.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, filename: str) -&gt; None:

    &#34;&#34;&#34;
    Loads the trained neural network from a HDF5 file.

    Parameters
    ----------
    filename : str
        Input filename.
    &#34;&#34;&#34;

    ################################################################################################################

    import h5py

    ################################################################################################################

    header_extra, dataset_extra = self._init_hdf5_extra()

    ################################################################################################################

    with h5py.File(filename, mode = &#39;r&#39;) as file:

        model_group = file[&#39;model&#39;]

        ############################################################################################################
        # HEADERS                                                                                                  #
        ############################################################################################################

        for name, field in header_extra.items():

            try:

                value = model_group.attrs[name]

                if isinstance(value, np.int32) or isinstance(value, np.int64):

                    setattr(self, field, int(value))

                elif isinstance(value, np.float32) or isinstance(value, np.float64):

                    setattr(self, field, float(value))

                else:

                    setattr(self, field, value)

            except KeyError:

                pass

        ############################################################################################################
        # DATASETS                                                                                                 #
        ############################################################################################################

        for name, field in dataset_extra.items():

            try:

                array = np.array(model_group[name])

                array = array.astype(self._dtype)

                setattr(self, field, array)

            except KeyError:

                pass

    ################################################################################################################

    self._rebuild_topography()</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_weights"><code class="name flex">
<span>def <span class="ident">get_weights</span></span>(<span>self) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the neural network weights with the shape [m * n, dim].</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_weights(self) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns the neural network weights with the shape [m * n, dim].
    &#34;&#34;&#34;

    return self._weights.reshape((self._m * self._n, self._dim))</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_centroids"><code class="name flex">
<span>def <span class="ident">get_centroids</span></span>(<span>self) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the neural network weights with the shape [m, n, dim].</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_centroids(self) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns the neural network weights with the shape [m, n, dim].
    &#34;&#34;&#34;

    return self._weights.reshape((self._m, self._n, self._dim))</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_quantization_errors"><code class="name flex">
<span>def <span class="ident">get_quantization_errors</span></span>(<span>self) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the quantization error. <span><span class="MathJax_Preview"> c_i^1\equiv\mathrm{1^\mathrm{st}\,bmu}\equiv\underset{j}{\mathrm{arg\,min}_1}\lVert x_i-w_j\rVert </span><script type="math/tex; mode=display"> c_i^1\equiv\mathrm{1^\mathrm{st}\,bmu}\equiv\underset{j}{\mathrm{arg\,min}_1}\lVert x_i-w_j\rVert </script></span> <span><span class="MathJax_Preview"> \boxed{e_Q\equiv\frac{1}{N}\sum_{i=1}^N\lVert x_i-w_{c_i^1}\rVert} </span><script type="math/tex; mode=display"> \boxed{e_Q\equiv\frac{1}{N}\sum_{i=1}^N\lVert x_i-w_{c_i^1}\rVert} </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_quantization_errors(self) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns the quantization error. $$ c_i^1\\equiv\\mathrm{1^\\mathrm{st}\\,bmu}\\equiv\\underset{j}{\\mathrm{arg\\,min}_1}\\lVert x_i-w_j\\rVert $$ $$ \\boxed{e_Q\\equiv\\frac{1}{N}\\sum_{i=1}^N\\lVert x_i-w_{c_i^1}\\rVert} $$
    &#34;&#34;&#34;

    return self._quantization_errors</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_topographic_errors"><code class="name flex">
<span>def <span class="ident">get_topographic_errors</span></span>(<span>self) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the topographic errors. <span><span class="MathJax_Preview"> c_i^n\equiv\mathrm{n^\mathrm{th}\,bmu}\equiv\underset{j}{\mathrm{arg\,min}_n}\lVert x_i-w_j\rVert </span><script type="math/tex; mode=display"> c_i^n\equiv\mathrm{n^\mathrm{th}\,bmu}\equiv\underset{j}{\mathrm{arg\,min}_n}\lVert x_i-w_j\rVert </script></span> <span><span class="MathJax_Preview"> r\equiv\left\{\begin{array}{ll}\sqrt{1}&amp;\mathrm{topology=hexagon}\\\sqrt{2}&amp;\mathrm{topology=square}\end{array}\right. </span><script type="math/tex; mode=display"> r\equiv\left\{\begin{array}{ll}\sqrt{1}&\mathrm{topology=hexagon}\\\sqrt{2}&\mathrm{topology=square}\end{array}\right. </script></span> <span><span class="MathJax_Preview"> t(x_i)\equiv\left\{\begin{array}{ll}1&amp;\lVert c_i^1-c_i^2\rVert&gt;r\\0&amp;\mathrm{otherwise}\end{array}\right. </span><script type="math/tex; mode=display"> t(x_i)\equiv\left\{\begin{array}{ll}1&\lVert c_i^1-c_i^2\rVert>r\\0&\mathrm{otherwise}\end{array}\right. </script></span> <span><span class="MathJax_Preview"> \boxed{e_t\equiv\frac{1}{N}\sum_{i=0}^Nt(x_i)} </span><script type="math/tex; mode=display"> \boxed{e_t\equiv\frac{1}{N}\sum_{i=0}^Nt(x_i)} </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_topographic_errors(self) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns the topographic errors. $$ c_i^n\\equiv\\mathrm{n^\\mathrm{th}\\,bmu}\\equiv\\underset{j}{\\mathrm{arg\\,min}_n}\\lVert x_i-w_j\\rVert $$ $$ r\\equiv\\left\\{\\begin{array}{ll}\\sqrt{1}&amp;\\mathrm{topology=hexagon}\\\\\\sqrt{2}&amp;\\mathrm{topology=square}\\end{array}\\right. $$ $$ t(x_i)\\equiv\\left\\{\\begin{array}{ll}1&amp;\\lVert c_i^1-c_i^2\\rVert&gt;r\\\\0&amp;\\mathrm{otherwise}\\end{array}\\right. $$ $$ \\boxed{e_t\\equiv\\frac{1}{N}\\sum_{i=0}^Nt(x_i)} $$
    &#34;&#34;&#34;

    return self._topographic_errors</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_distance_map"><code class="name flex">
<span>def <span class="ident">get_distance_map</span></span>(<span>self, scaling:Â Optional[str]Â =Â None) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a matrix of the distances between each weight.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>scaling</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Normalization method, either '<strong>sum</strong>' or '<strong>mean</strong>' (default: <strong>None</strong>, uses: '<strong>sum</strong>').</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_distance_map(self, scaling: Optional[str] = None) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns a matrix of the distances between each weight.

    Parameters
    ----------
    scaling : Optional[str]
        Normalization method, either &#39;**sum**&#39; or &#39;**mean**&#39; (default: **None**, uses: &#39;**sum**&#39;).
    &#34;&#34;&#34;

    scaling = scaling or &#39;sum&#39;

    ################################################################################################################

    if self._topology == &#39;square&#39;:

        result = np.full((self._m, self._n, 8), np.nan, dtype = self._dtype)

        SOM_Abstract._distance_map(result, self.get_centroids(), SOM_Abstract._X_SQU_STENCIL, SOM_Abstract._Y_SQU_STENCIL, self._m, self._n, 8)

    else:

        result = np.full((self._m, self._n, 6), np.nan, dtype = self._dtype)

        SOM_Abstract._distance_map(result, self.get_centroids(), SOM_Abstract._X_HEX_STENCIL, SOM_Abstract._Y_HEX_STENCIL, self._m, self._n, 6)

    ################################################################################################################

    if scaling == &#39;sum&#39;:
        result = np.nansum(result, axis = 2)

    elif scaling == &#39;mean&#39;:
        result = np.nanmean(result, axis = 2)

    else:
        raise Exception(f&#39;Invalid scaling method `{scaling}`&#39;)

    ################################################################################################################

    return result / np.max(result)</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_activation_map"><code class="name flex">
<span>def <span class="ident">get_activation_map</span></span>(<span>self, dataset:Â Union[np.ndarray,Â Callable], enable_gpu:Â boolÂ =Â True, threads_per_blocks:Â intÂ =Â 1024) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a matrix containing the number of times each neuron have been activated for the given input.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Union[np.ndarray, Callable]</code></dt>
<dd>Dataset array or generator builder.</dd>
<dt><strong><code>enable_gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>If available, run on GPU rather than CPU (default: <strong>True</strong>).</dd>
<dt><strong><code>threads_per_blocks</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of GPU threads per blocks (default: <strong>1024</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_activation_map(self, dataset: Union[np.ndarray, Callable], enable_gpu: bool = True, threads_per_blocks: int = 1024) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns a matrix containing the number of times each neuron have been activated for the given input.

    Parameters
    ----------
    dataset : Union[np.ndarray, Callable]
        Dataset array or generator builder.
    enable_gpu : bool
        If available, run on GPU rather than CPU (default: **True**).
    threads_per_blocks : int
        Number of GPU threads per blocks (default: **1024**).
    &#34;&#34;&#34;

    ################################################################################################################

    generator_builder = dataset_to_generator_builder(dataset)

    ################################################################################################################

    result = device_array_zeros(shape = (self._m * self._n, ), dtype = np.int64)

    ################################################################################################################

    generator = generator_builder()

    for data in generator():

        _count_bmus_kernel[enable_gpu, threads_per_blocks, data.shape[0]](result, self._weights, data, self._m * self._n)

    ################################################################################################################

    return result.copy_to_host().reshape((self._m, self._n))</code></pre>
</details>
</dd>
<dt id="decontamination.SOM_Abstract.get_winners"><code class="name flex">
<span>def <span class="ident">get_winners</span></span>(<span>self, dataset:Â np.ndarray, enable_gpu:Â boolÂ =Â True, threads_per_blocks:Â intÂ =Â 1024) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a vector of the best matching unit indices (shape mÃ—n) for the given input.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Dataset array.</dd>
<dt><strong><code>enable_gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>If available, run on GPU rather than CPU (default: <strong>True</strong>).</dd>
<dt><strong><code>threads_per_blocks</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of GPU threads per blocks (default: <strong>1024</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_winners(self, dataset: np.ndarray, enable_gpu: bool = True, threads_per_blocks: int = 1024) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Returns a vector of the best matching unit indices (shape mÃ—n) for the given input.

    Parameters
    ----------
    dataset : np.ndarray
        Dataset array.
    enable_gpu : bool
        If available, run on GPU rather than CPU (default: **True**).
    threads_per_blocks : int
        Number of GPU threads per blocks (default: **1024**).
    &#34;&#34;&#34;

    ################################################################################################################

    result = device_array_empty(dataset.shape[0], dtype = np.int32)

    ################################################################################################################

    _find_bmus_kernel[enable_gpu, threads_per_blocks, dataset.shape[0]](result, self._weights, dataset, self._m * self._n)

    ################################################################################################################

    return result.copy_to_host()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="decontamination.SOM_PCA"><code class="flex name class">
<span>class <span class="ident">SOM_PCA</span></span>
<span>(</span><span>m:Â int, n:Â int, dim:Â int, dtype:Â Type[np.float32]Â =Â np.float32, topology:Â Optional[str]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Self Organizing Maps that span the first two principal components.</p>
<p>A rule of thumb to set the size of the grid for a dimensionality reduction task is that it should contain <span><span class="MathJax_Preview"> 5\sqrt{N} </span><script type="math/tex"> 5\sqrt{N} </script></span> neurons where N is the number of samples in the dataset to analyze.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron rows.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron columns.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of the input data.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Neural network data type (default: <strong>np.float32</strong>).</dd>
<dt><strong><code>topology</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Topology of the map, either '<strong>square</strong>' or '<strong>hexagonal</strong>' (default: <strong>None</strong>, uses: '<strong>hexagonal</strong>').</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SOM_PCA(som_abstract.SOM_Abstract):

    &#34;&#34;&#34;
    Self Organizing Maps that span the first two principal components.
    &#34;&#34;&#34;

    __MODE__ = &#39;pca&#39;

    ####################################################################################################################

    def __init__(self, m: int, n: int, dim: int, dtype: Type[np.single] = np.float32, topology: Optional[str] = None):

        &#34;&#34;&#34;
        A rule of thumb to set the size of the grid for a dimensionality reduction task is that it should contain \\( 5\\sqrt{N} \\) neurons where N is the number of samples in the dataset to analyze.

        Parameters
        ----------
        m : int
            Number of neuron rows.
        n : int
            Number of neuron columns.
        dim : int
            Dimensionality of the input data.
        dtype : Type[np.single]
            Neural network data type (default: **np.float32**).
        topology : Optional[str]
            Topology of the map, either &#39;**square**&#39; or &#39;**hexagonal**&#39; (default: **None**, uses: &#39;**hexagonal**&#39;).
        &#34;&#34;&#34;

        ################################################################################################################

        super().__init__(m, n, dim, dtype, topology)

        ################################################################################################################

        self._header_extra = {
            &#39;mode&#39;: &#39;__MODE__&#39;,
        }

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _update_cov_matrix(result_sum: np.ndarray, result_prods: np.ndarray, data: np.ndarray) -&gt; None:

        ################################################################################################################

        data_dim = data.shape[0]
        syst_dim = data.shape[1]

        ################################################################################################################

        for i in range(data_dim):

            value = data[i].astype(np.float64)

            for j in range(syst_dim):

                value_j = value[j]
                result_sum[j] += value_j

                for k in range(syst_dim):

                    value_jk = value_j * value[k]
                    result_prods[j][k] += value_jk

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _diag_cov_matrix(weights: np.ndarray, cov_matrix: np.ndarray, min_weight: float, max_weight: float, m: int, n: int) -&gt; None:

        ################################################################################################################

        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

        orders = np.argsort(-eigenvalues)

        order0 = orders[0]
        order1 = orders[1]

        ################################################################################################################

        linspace_x = np.linspace(min_weight, max_weight, m)
        linspace_y = np.linspace(min_weight, max_weight, n)

        for i in range(m):
            c1 = linspace_x[i]

            for j in range(n):
                c2 = linspace_y[j]

                weights[i, j] = (
                    eigenvectors[:, order0] * c1
                    +
                    eigenvectors[:, order1] * c2
                ).astype(weights.dtype)

    ####################################################################################################################

    def train(self, dataset: Union[np.ndarray, Callable], min_weight: float = 0.0, max_weight: float = 1.0) -&gt; None:

        &#34;&#34;&#34;
        Trains the neural network.

        Parameters
        ----------
        dataset : Union[np.ndarray, Callable]
            Training dataset array or generator builder.
        min_weight : float
            Latent space minimum value (default: **O.O**).
        max_weight : float
            Latent space maximum value (default: **1.O**).
        &#34;&#34;&#34;

        ################################################################################################################

        generator_builder = dataset_to_generator_builder(dataset)

        generator = generator_builder()

        ################################################################################################################

        total_nb = 0

        total_sum = np.zeros((self._dim, ), dtype = np.float64)
        total_prods = np.zeros((self._dim, self._dim), dtype = np.float64)

        ################################################################################################################

        for data in generator():

            total_nb += data.shape[0]

            SOM_PCA._update_cov_matrix(total_sum, total_prods, data)

        ################################################################################################################

        total_sum /= total_nb
        total_prods /= total_nb

        ################################################################################################################

        cov_matrix = total_prods - np.outer(total_sum, total_sum)

        ################################################################################################################

        SOM_PCA._diag_cov_matrix(self.get_centroids(), cov_matrix, min_weight, max_weight, self._m, self._n)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>decontamination._algo.som_abstract.SOM_Abstract</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="decontamination.SOM_PCA.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, dataset:Â Union[np.ndarray,Â Callable], min_weight:Â floatÂ =Â 0.0, max_weight:Â floatÂ =Â 1.0) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the neural network.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Union[np.ndarray, Callable]</code></dt>
<dd>Training dataset array or generator builder.</dd>
<dt><strong><code>min_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Latent space minimum value (default: <strong>O.O</strong>).</dd>
<dt><strong><code>max_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Latent space maximum value (default: <strong>1.O</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, dataset: Union[np.ndarray, Callable], min_weight: float = 0.0, max_weight: float = 1.0) -&gt; None:

    &#34;&#34;&#34;
    Trains the neural network.

    Parameters
    ----------
    dataset : Union[np.ndarray, Callable]
        Training dataset array or generator builder.
    min_weight : float
        Latent space minimum value (default: **O.O**).
    max_weight : float
        Latent space maximum value (default: **1.O**).
    &#34;&#34;&#34;

    ################################################################################################################

    generator_builder = dataset_to_generator_builder(dataset)

    generator = generator_builder()

    ################################################################################################################

    total_nb = 0

    total_sum = np.zeros((self._dim, ), dtype = np.float64)
    total_prods = np.zeros((self._dim, self._dim), dtype = np.float64)

    ################################################################################################################

    for data in generator():

        total_nb += data.shape[0]

        SOM_PCA._update_cov_matrix(total_sum, total_prods, data)

    ################################################################################################################

    total_sum /= total_nb
    total_prods /= total_nb

    ################################################################################################################

    cov_matrix = total_prods - np.outer(total_sum, total_sum)

    ################################################################################################################

    SOM_PCA._diag_cov_matrix(self.get_centroids(), cov_matrix, min_weight, max_weight, self._m, self._n)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="decontamination.SOM_Batch"><code class="flex name class">
<span>class <span class="ident">SOM_Batch</span></span>
<span>(</span><span>m:Â int, n:Â int, dim:Â int, dtype:Â Type[np.float32]Â =Â np.float32, topology:Â Optional[str]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Self Organizing Maps (standard batch implementation).</p>
<p>A rule of thumb to set the size of the grid for a dimensionality reduction task is that it should contain <span><span class="MathJax_Preview"> 5\sqrt{N} </span><script type="math/tex"> 5\sqrt{N} </script></span> neurons where N is the number of samples in the dataset to analyze.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron rows.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron columns.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of the input data.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Neural network data type (default: <strong>np.float32</strong>).</dd>
<dt><strong><code>topology</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Topology of the model, either <strong>'square'</strong> or <strong>'hexagonal'</strong> (default: <strong>None</strong>, uses: <strong>'hexagonal'</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SOM_Batch(som_abstract.SOM_Abstract):

    &#34;&#34;&#34;
    Self Organizing Maps (standard batch implementation).
    &#34;&#34;&#34;

    __MODE__ = &#39;batch&#39;

    ####################################################################################################################

    def __init__(self, m: int, n: int, dim: int, dtype: Type[np.single] = np.float32, topology: Optional[str] = None):

        &#34;&#34;&#34;
        A rule of thumb to set the size of the grid for a dimensionality reduction task is that it should contain \\( 5\\sqrt{N} \\) neurons where N is the number of samples in the dataset to analyze.

        Parameters
        ----------
        m : int
            Number of neuron rows.
        n : int
            Number of neuron columns.
        dim : int
            Dimensionality of the input data.
        dtype : Type[np.single]
            Neural network data type (default: **np.float32**).
        topology : Optional[str]
            Topology of the model, either **&#39;square&#39;** or **&#39;hexagonal&#39;** (default: **None**, uses: **&#39;hexagonal&#39;**).
        &#34;&#34;&#34;

        ################################################################################################################

        super().__init__(m, n, dim, dtype, topology)

        ################################################################################################################

        self._n_epochs = None

        ################################################################################################################

        self._header_extra = {
            &#39;mode&#39;: &#39;__MODE__&#39;,
            &#39;n_epochs&#39;: &#39;_n_epochs&#39;,
        }

    ####################################################################################################################

    def train(self, dataset: Union[np.ndarray, Callable], n_epochs: int, show_progress_bar: bool = False, enable_gpu: bool = True, threads_per_blocks: int = 1024) -&gt; None:

        &#34;&#34;&#34;
        Trains the neural network. Use either the &#34;*number of epochs*&#34; training method by specifying `n_epochs` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_epochs}\\}-1 \\)) or the &#34;*number of vectors*&#34; training method by specifying `n_vectors` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_vectors}\\}-1 \\)). A batch formulation of updating weights is implemented: $$ c_i(w,e)\\equiv\\mathrm{bmu}(x_i,w,e)\\equiv\\underset{j}{\\mathrm{arg\\,min}}\\lVert x_i-w_j(e)\\rVert $$ $$ \\Theta_{ji}(w,e)\\equiv\\delta_{j,c_i(w,e)}\\equiv\\left\\{\\begin{array}{ll}1&amp;j=c_i(w,e)\\\\0&amp;\\mathrm{otherwise}\\end{array}\\right. $$ $$ \\boxed{w_j(e+1)=\\frac{\\sum_{i=0}^{N-1}\\Theta_{ji}(w,e)x_i}{\\sum_{i=0}^{N-1}\\Theta_{ji}(w,e)}} $$ where \\( j=0\\dots m\\times n-1 \\).

        Parameters
        ----------
        dataset : Union[np.ndarray, Callable]
            Training dataset array or generator builder.
        n_epochs : int
            Number of epochs to train for.
        show_progress_bar : bool
            Specifies whether to display a progress bar (default: **False**).
        enable_gpu : bool
            If available, run on GPU rather than CPU (default: **True**).
        threads_per_blocks : int
            Number of GPU threads per blocks (default: **1024**).
        &#34;&#34;&#34;

        ################################################################################################################

        generator_builder = dataset_to_generator_builder(dataset)

        ################################################################################################################

        cur_vector = 0

        self._n_epochs = n_epochs

        penalty_dist = 2.0 if self._topology == &#39;square&#39; else 1.0

        ################################################################################################################
        # TRAINING BY NUMBER OF EPOCHS                                                                                 #
        ################################################################################################################

        quantization_errors = device_array_empty(n_epochs, dtype = np.float32)

        topographic_errors = device_array_empty(n_epochs, dtype = np.float32)

        ################################################################################################################

        for cur_epoch in tqdm.trange(n_epochs, disable = not show_progress_bar):

            ############################################################################################################

            numerator = device_array_zeros(shape = (self._m * self._n, self._dim), dtype = self._dtype)

            denominator = device_array_zeros(shape = (self._m * self._n, 1), dtype = self._dtype)

            ############################################################################################################

            generator = generator_builder()

            for data in generator():

                cur_vector += data.shape[0]

                _train_kernel[enable_gpu, threads_per_blocks, data.shape[0]](
                    numerator,
                    denominator,
                    quantization_errors,
                    topographic_errors,
                    self._weights,
                    self._topography,
                    data,
                    penalty_dist,
                    cur_epoch,
                    self._m * self._n
                )

            ############################################################################################################

            self._weights = np.divide(
                numerator.copy_to_host(),
                denominator.copy_to_host(),
                out = np.zeros(numerator.shape, dtype = np.float32),
                where = denominator != 0.0
            )

        ################################################################################################################

        if cur_vector &gt; 0:

            self._quantization_errors = quantization_errors.copy_to_host() * n_epochs / cur_vector

            self._topographic_errors = topographic_errors.copy_to_host() * n_epochs / cur_vector</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>decontamination._algo.som_abstract.SOM_Abstract</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="decontamination.SOM_Batch.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, dataset:Â Union[np.ndarray,Â Callable], n_epochs:Â int, show_progress_bar:Â boolÂ =Â False, enable_gpu:Â boolÂ =Â True, threads_per_blocks:Â intÂ =Â 1024) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the neural network. Use either the "<em>number of epochs</em>" training method by specifying <code>n_epochs</code> (then <span><span class="MathJax_Preview"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_epochs}\}-1 </span><script type="math/tex"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_epochs}\}-1 </script></span>) or the "<em>number of vectors</em>" training method by specifying <code>n_vectors</code> (then <span><span class="MathJax_Preview"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_vectors}\}-1 </span><script type="math/tex"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_vectors}\}-1 </script></span>). A batch formulation of updating weights is implemented: <span><span class="MathJax_Preview"> c_i(w,e)\equiv\mathrm{bmu}(x_i,w,e)\equiv\underset{j}{\mathrm{arg\,min}}\lVert x_i-w_j(e)\rVert </span><script type="math/tex; mode=display"> c_i(w,e)\equiv\mathrm{bmu}(x_i,w,e)\equiv\underset{j}{\mathrm{arg\,min}}\lVert x_i-w_j(e)\rVert </script></span> <span><span class="MathJax_Preview"> \Theta_{ji}(w,e)\equiv\delta_{j,c_i(w,e)}\equiv\left\{\begin{array}{ll}1&amp;j=c_i(w,e)\\0&amp;\mathrm{otherwise}\end{array}\right. </span><script type="math/tex; mode=display"> \Theta_{ji}(w,e)\equiv\delta_{j,c_i(w,e)}\equiv\left\{\begin{array}{ll}1&j=c_i(w,e)\\0&\mathrm{otherwise}\end{array}\right. </script></span> <span><span class="MathJax_Preview"> \boxed{w_j(e+1)=\frac{\sum_{i=0}^{N-1}\Theta_{ji}(w,e)x_i}{\sum_{i=0}^{N-1}\Theta_{ji}(w,e)}} </span><script type="math/tex; mode=display"> \boxed{w_j(e+1)=\frac{\sum_{i=0}^{N-1}\Theta_{ji}(w,e)x_i}{\sum_{i=0}^{N-1}\Theta_{ji}(w,e)}} </script></span> where <span><span class="MathJax_Preview"> j=0\dots m\times n-1 </span><script type="math/tex"> j=0\dots m\times n-1 </script></span>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Union[np.ndarray, Callable]</code></dt>
<dd>Training dataset array or generator builder.</dd>
<dt><strong><code>n_epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of epochs to train for.</dd>
<dt><strong><code>show_progress_bar</code></strong> :&ensp;<code>bool</code></dt>
<dd>Specifies whether to display a progress bar (default: <strong>False</strong>).</dd>
<dt><strong><code>enable_gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>If available, run on GPU rather than CPU (default: <strong>True</strong>).</dd>
<dt><strong><code>threads_per_blocks</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of GPU threads per blocks (default: <strong>1024</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, dataset: Union[np.ndarray, Callable], n_epochs: int, show_progress_bar: bool = False, enable_gpu: bool = True, threads_per_blocks: int = 1024) -&gt; None:

    &#34;&#34;&#34;
    Trains the neural network. Use either the &#34;*number of epochs*&#34; training method by specifying `n_epochs` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_epochs}\\}-1 \\)) or the &#34;*number of vectors*&#34; training method by specifying `n_vectors` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_vectors}\\}-1 \\)). A batch formulation of updating weights is implemented: $$ c_i(w,e)\\equiv\\mathrm{bmu}(x_i,w,e)\\equiv\\underset{j}{\\mathrm{arg\\,min}}\\lVert x_i-w_j(e)\\rVert $$ $$ \\Theta_{ji}(w,e)\\equiv\\delta_{j,c_i(w,e)}\\equiv\\left\\{\\begin{array}{ll}1&amp;j=c_i(w,e)\\\\0&amp;\\mathrm{otherwise}\\end{array}\\right. $$ $$ \\boxed{w_j(e+1)=\\frac{\\sum_{i=0}^{N-1}\\Theta_{ji}(w,e)x_i}{\\sum_{i=0}^{N-1}\\Theta_{ji}(w,e)}} $$ where \\( j=0\\dots m\\times n-1 \\).

    Parameters
    ----------
    dataset : Union[np.ndarray, Callable]
        Training dataset array or generator builder.
    n_epochs : int
        Number of epochs to train for.
    show_progress_bar : bool
        Specifies whether to display a progress bar (default: **False**).
    enable_gpu : bool
        If available, run on GPU rather than CPU (default: **True**).
    threads_per_blocks : int
        Number of GPU threads per blocks (default: **1024**).
    &#34;&#34;&#34;

    ################################################################################################################

    generator_builder = dataset_to_generator_builder(dataset)

    ################################################################################################################

    cur_vector = 0

    self._n_epochs = n_epochs

    penalty_dist = 2.0 if self._topology == &#39;square&#39; else 1.0

    ################################################################################################################
    # TRAINING BY NUMBER OF EPOCHS                                                                                 #
    ################################################################################################################

    quantization_errors = device_array_empty(n_epochs, dtype = np.float32)

    topographic_errors = device_array_empty(n_epochs, dtype = np.float32)

    ################################################################################################################

    for cur_epoch in tqdm.trange(n_epochs, disable = not show_progress_bar):

        ############################################################################################################

        numerator = device_array_zeros(shape = (self._m * self._n, self._dim), dtype = self._dtype)

        denominator = device_array_zeros(shape = (self._m * self._n, 1), dtype = self._dtype)

        ############################################################################################################

        generator = generator_builder()

        for data in generator():

            cur_vector += data.shape[0]

            _train_kernel[enable_gpu, threads_per_blocks, data.shape[0]](
                numerator,
                denominator,
                quantization_errors,
                topographic_errors,
                self._weights,
                self._topography,
                data,
                penalty_dist,
                cur_epoch,
                self._m * self._n
            )

        ############################################################################################################

        self._weights = np.divide(
            numerator.copy_to_host(),
            denominator.copy_to_host(),
            out = np.zeros(numerator.shape, dtype = np.float32),
            where = denominator != 0.0
        )

    ################################################################################################################

    if cur_vector &gt; 0:

        self._quantization_errors = quantization_errors.copy_to_host() * n_epochs / cur_vector

        self._topographic_errors = topographic_errors.copy_to_host() * n_epochs / cur_vector</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="decontamination.SOM_Online"><code class="flex name class">
<span>class <span class="ident">SOM_Online</span></span>
<span>(</span><span>m:Â int, n:Â int, dim:Â int, dtype:Â Type[np.float32]Â =Â np.float32, topology:Â Optional[str]Â =Â None, alpha:Â floatÂ =Â None, sigma:Â floatÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Self Organizing Maps (standard online implementation).</p>
<p>A rule of thumb to set the size of the grid for a dimensionality reduction task is that it should contain <span><span class="MathJax_Preview"> 5\sqrt{N} </span><script type="math/tex"> 5\sqrt{N} </script></span> neurons where N is the number of samples in the dataset to analyze.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron rows.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neuron columns.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of the input data.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type[np.single]</code></dt>
<dd>Neural network data type (default: <strong>np.float32</strong>).</dd>
<dt><strong><code>topology</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Topology of the model, either <strong>'square'</strong> or <strong>'hexagonal'</strong> (default: <strong>None</strong>, uses: <strong>'hexagonal'</strong>).</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Starting value of the learning rate (default: <strong>None</strong>, uses: <strong>0.3</strong>).</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>Starting value of the neighborhood radius (default: <strong>None</strong>, uses: <span><span class="MathJax_Preview"> \mathrm{max}(m,n)/2 </span><script type="math/tex"> \mathrm{max}(m,n)/2 </script></span>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SOM_Online(som_abstract.SOM_Abstract):

    &#34;&#34;&#34;
    Self Organizing Maps (standard online implementation).
    &#34;&#34;&#34;

    __MODE__ = &#39;online&#39;

    ####################################################################################################################

    def __init__(self, m: int, n: int, dim: int, dtype: Type[np.single] = np.float32, topology: Optional[str] = None, alpha: float = None, sigma: float = None):

        &#34;&#34;&#34;
        A rule of thumb to set the size of the grid for a dimensionality reduction task is that it should contain \\( 5\\sqrt{N} \\) neurons where N is the number of samples in the dataset to analyze.

        Parameters
        ----------
        m : int
            Number of neuron rows.
        n : int
            Number of neuron columns.
        dim : int
            Dimensionality of the input data.
        dtype : Type[np.single]
            Neural network data type (default: **np.float32**).
        topology : Optional[str]
            Topology of the model, either **&#39;square&#39;** or **&#39;hexagonal&#39;** (default: **None**, uses: **&#39;hexagonal&#39;**).
        alpha : float
            Starting value of the learning rate (default: **None**, uses: **0.3**).
        sigma : float
            Starting value of the neighborhood radius (default: **None**, uses: \\( \\mathrm{max}(m,n)/2 \\)).
        &#34;&#34;&#34;

        ################################################################################################################

        super().__init__(m, n, dim, dtype, topology)

        ################################################################################################################

        self._alpha = 0.3 if alpha is None else float(alpha)

        self._sigma = max(m, n) / 2.0 if sigma is None else float(sigma)

        ################################################################################################################

        self._n_epochs = None

        self._n_vectors = None

        ################################################################################################################

        self._header_extra = {
            &#39;mode&#39;: &#39;__MODE__&#39;,
            &#39;alpha&#39;: &#39;_alpha&#39;,
            &#39;sigma&#39;: &#39;_sigma&#39;,
            &#39;n_epochs&#39;: &#39;_n_epochs&#39;,
            &#39;n_vectors&#39;: &#39;_n_vectors&#39;,
        }

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _train_step1_epoch(weights: np.ndarray, quantization_errors: np.ndarray, topographic_errors: np.ndarray, topography: np.ndarray, data: np.ndarray, cur_epoch: int, n_epochs: int, alpha0: float, sigma0: float, penalty_dist: float, mn: int):

        ################################################################################################################

        decay_function = _asymptotic_decay(cur_epoch, n_epochs)

        alpha = alpha0 * decay_function

        sigma = sigma0 * decay_function

        ################################################################################################################

        for i in range(data.shape[0]):

            _train_step2(quantization_errors, topographic_errors, weights, topography, data[i], alpha, sigma, penalty_dist, cur_epoch, mn)

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _train_step1_iter(weights: np.ndarray, quantization_errors: np.ndarray, topographic_errors: np.ndarray, topography: np.ndarray, data: np.ndarray, cur_vector: int, n_vectors: int, n_err_bins: int, alpha0: float, sigma0: float, penalty_dist: float, mn: int):

        for i in range(data.shape[0]):

            ############################################################################################################

            cur_err_bin = (n_err_bins * (cur_vector + i)) // n_vectors

            ############################################################################################################

            decay_function = _asymptotic_decay(cur_vector + i, n_vectors)

            alpha = alpha0 * decay_function

            sigma = sigma0 * decay_function

            ############################################################################################################

            _train_step2(quantization_errors, topographic_errors, weights, topography, data[i], alpha, sigma, penalty_dist, cur_err_bin, mn)

    ####################################################################################################################

    def train(self, dataset: Union[np.ndarray, Callable], n_epochs: Optional[int] = None, n_vectors: Optional[int] = None, n_error_bins: Optional[int] = 10, show_progress_bar: bool = False) -&gt; None:

        &#34;&#34;&#34;
        Trains the neural network. Use either the &#34;*number of epochs*&#34; training method by specifying `n_epochs` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_epochs}\\}-1 \\)) or the &#34;*number of vectors*&#34; training method by specifying `n_vectors` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_vectors}\\}-1 \\)). An online formulation of updating weights is implemented: $$ c_i(w,e)\\equiv\\mathrm{bmu}(x_i,w,e)\\equiv\\underset{j}{\\mathrm{arg\\,min}}\\lVert x_i-w_j(e)\\rVert $$ $$ \\Theta_{ji}(w,e)\\equiv\\alpha(e)\\cdot\\exp\\left(-\\frac{\\lVert j-c_i(w,e)\\rVert}{2\\sigma^2(e)}\\right) $$ $$ \\boxed{\\mathrm{iteratively\\,for}\\,i=0\\dots N-1\\,\\mathrm{:}\\,w_j(e+1)=w_j(e)+\\Theta_{ji}(w,e)[x_i-w_j(e)]} $$ where \\( j=0\\dots m\\times n-1 \\) and, at epoch \\( e \\), \\( \\alpha(e)\\equiv\\alpha\\cdot\\frac{1}{1+2\\frac{e}{e_\\mathrm{tot}}} \\) is the learning rate and \\( \\sigma(e)\\equiv\\sigma\\cdot\\frac{1}{1+2\\frac{e}{e_\\mathrm{tot}}} \\) is the neighborhood radius.

        Parameters
        ----------
        dataset : Union[np.ndarray, Callable]
            Training dataset array or generator builder.
        n_epochs : Optional[int]
            Number of epochs to train for (default: **None**).
        n_vectors : Optional[int]
            Number of vectors to train for (default: **None**).
        n_error_bins : int
            Number of quantization and topographic error bins (default: **10**).
        show_progress_bar : bool
            Specifies whether to display a progress bar (default: **False**).
        &#34;&#34;&#34;

        ################################################################################################################

        generator_builder = dataset_to_generator_builder(dataset)

        ################################################################################################################

        cur_vector = 0

        self._n_epochs = n_epochs

        self._n_vectors = n_vectors

        penalty_dist = 2.0 if self._topology == &#39;square&#39; else 1.0

        if not (n_epochs is None) and (n_vectors is None):

            ############################################################################################################
            # TRAINING BY NUMBER OF EPOCHS                                                                             #
            ############################################################################################################

            self._quantization_errors = np.zeros(n_epochs, dtype = np.float32)

            self._topographic_errors = np.zeros(n_epochs, dtype = np.float32)

            ############################################################################################################

            for cur_epoch in tqdm.trange(n_epochs, disable = not show_progress_bar):

                generator = generator_builder()

                for data in generator():

                    cur_vector += data.shape[0]

                    SOM_Online._train_step1_epoch(
                        self._weights,
                        self._quantization_errors,
                        self._topographic_errors,
                        self._topography,
                        data,
                        cur_epoch,
                        n_epochs,
                        self._alpha,
                        self._sigma,
                        penalty_dist,
                        self._m * self._n
                    )

            ############################################################################################################

            if cur_vector &gt; 0:

                self._quantization_errors = self._quantization_errors * n_epochs / cur_vector

                self._topographic_errors = self._topographic_errors * n_epochs / cur_vector

            ############################################################################################################

        elif (n_epochs is None) and not (n_vectors is None):

            ############################################################################################################
            # TRAINING BY NUMBER OF VECTORS                                                                            #
            ############################################################################################################

            self._quantization_errors = np.zeros(n_error_bins, dtype = np.float32)

            self._topographic_errors = np.zeros(n_error_bins, dtype = np.float32)

            ############################################################################################################

            progress_bar = tqdm.tqdm(total = n_vectors, disable = not show_progress_bar)

            generator = generator_builder()

            for data in generator():

                count = min(data.shape[0], n_vectors - cur_vector)

                SOM_Online._train_step1_iter(
                    self._weights,
                    self._quantization_errors,
                    self._topographic_errors,
                    self._topography,
                    data[0: count],
                    cur_vector,
                    n_vectors,
                    n_error_bins,
                    self._alpha,
                    self._sigma,
                    penalty_dist,
                    self._m * self._n
                )

                cur_vector += count

                progress_bar.update(count)

                if cur_vector &gt;= n_vectors:

                    break

            ############################################################################################################

            if cur_vector &gt; 0:

                self._quantization_errors = self._quantization_errors * n_error_bins / cur_vector

                self._topographic_errors = self._topographic_errors * n_error_bins / cur_vector

            ############################################################################################################

        else:

            raise Exception(&#39;Invalid training method, specify either `n_epochs` or `n_vectors`.&#39;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>decontamination._algo.som_abstract.SOM_Abstract</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="decontamination.SOM_Online.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, dataset:Â Union[np.ndarray,Â Callable], n_epochs:Â Optional[int]Â =Â None, n_vectors:Â Optional[int]Â =Â None, n_error_bins:Â Optional[int]Â =Â 10, show_progress_bar:Â boolÂ =Â False) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the neural network. Use either the "<em>number of epochs</em>" training method by specifying <code>n_epochs</code> (then <span><span class="MathJax_Preview"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_epochs}\}-1 </span><script type="math/tex"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_epochs}\}-1 </script></span>) or the "<em>number of vectors</em>" training method by specifying <code>n_vectors</code> (then <span><span class="MathJax_Preview"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_vectors}\}-1 </span><script type="math/tex"> e\equiv 0\dots\{e_\mathrm{tot}\equiv\mathrm{n\_vectors}\}-1 </script></span>). An online formulation of updating weights is implemented: <span><span class="MathJax_Preview"> c_i(w,e)\equiv\mathrm{bmu}(x_i,w,e)\equiv\underset{j}{\mathrm{arg\,min}}\lVert x_i-w_j(e)\rVert </span><script type="math/tex; mode=display"> c_i(w,e)\equiv\mathrm{bmu}(x_i,w,e)\equiv\underset{j}{\mathrm{arg\,min}}\lVert x_i-w_j(e)\rVert </script></span> <span><span class="MathJax_Preview"> \Theta_{ji}(w,e)\equiv\alpha(e)\cdot\exp\left(-\frac{\lVert j-c_i(w,e)\rVert}{2\sigma^2(e)}\right) </span><script type="math/tex; mode=display"> \Theta_{ji}(w,e)\equiv\alpha(e)\cdot\exp\left(-\frac{\lVert j-c_i(w,e)\rVert}{2\sigma^2(e)}\right) </script></span> <span><span class="MathJax_Preview"> \boxed{\mathrm{iteratively\,for}\,i=0\dots N-1\,\mathrm{:}\,w_j(e+1)=w_j(e)+\Theta_{ji}(w,e)[x_i-w_j(e)]} </span><script type="math/tex; mode=display"> \boxed{\mathrm{iteratively\,for}\,i=0\dots N-1\,\mathrm{:}\,w_j(e+1)=w_j(e)+\Theta_{ji}(w,e)[x_i-w_j(e)]} </script></span> where <span><span class="MathJax_Preview"> j=0\dots m\times n-1 </span><script type="math/tex"> j=0\dots m\times n-1 </script></span> and, at epoch <span><span class="MathJax_Preview"> e </span><script type="math/tex"> e </script></span>, <span><span class="MathJax_Preview"> \alpha(e)\equiv\alpha\cdot\frac{1}{1+2\frac{e}{e_\mathrm{tot}}} </span><script type="math/tex"> \alpha(e)\equiv\alpha\cdot\frac{1}{1+2\frac{e}{e_\mathrm{tot}}} </script></span> is the learning rate and <span><span class="MathJax_Preview"> \sigma(e)\equiv\sigma\cdot\frac{1}{1+2\frac{e}{e_\mathrm{tot}}} </span><script type="math/tex"> \sigma(e)\equiv\sigma\cdot\frac{1}{1+2\frac{e}{e_\mathrm{tot}}} </script></span> is the neighborhood radius.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Union[np.ndarray, Callable]</code></dt>
<dd>Training dataset array or generator builder.</dd>
<dt><strong><code>n_epochs</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Number of epochs to train for (default: <strong>None</strong>).</dd>
<dt><strong><code>n_vectors</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Number of vectors to train for (default: <strong>None</strong>).</dd>
<dt><strong><code>n_error_bins</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quantization and topographic error bins (default: <strong>10</strong>).</dd>
<dt><strong><code>show_progress_bar</code></strong> :&ensp;<code>bool</code></dt>
<dd>Specifies whether to display a progress bar (default: <strong>False</strong>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, dataset: Union[np.ndarray, Callable], n_epochs: Optional[int] = None, n_vectors: Optional[int] = None, n_error_bins: Optional[int] = 10, show_progress_bar: bool = False) -&gt; None:

    &#34;&#34;&#34;
    Trains the neural network. Use either the &#34;*number of epochs*&#34; training method by specifying `n_epochs` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_epochs}\\}-1 \\)) or the &#34;*number of vectors*&#34; training method by specifying `n_vectors` (then \\( e\\equiv 0\\dots\\{e_\\mathrm{tot}\\equiv\\mathrm{n\\_vectors}\\}-1 \\)). An online formulation of updating weights is implemented: $$ c_i(w,e)\\equiv\\mathrm{bmu}(x_i,w,e)\\equiv\\underset{j}{\\mathrm{arg\\,min}}\\lVert x_i-w_j(e)\\rVert $$ $$ \\Theta_{ji}(w,e)\\equiv\\alpha(e)\\cdot\\exp\\left(-\\frac{\\lVert j-c_i(w,e)\\rVert}{2\\sigma^2(e)}\\right) $$ $$ \\boxed{\\mathrm{iteratively\\,for}\\,i=0\\dots N-1\\,\\mathrm{:}\\,w_j(e+1)=w_j(e)+\\Theta_{ji}(w,e)[x_i-w_j(e)]} $$ where \\( j=0\\dots m\\times n-1 \\) and, at epoch \\( e \\), \\( \\alpha(e)\\equiv\\alpha\\cdot\\frac{1}{1+2\\frac{e}{e_\\mathrm{tot}}} \\) is the learning rate and \\( \\sigma(e)\\equiv\\sigma\\cdot\\frac{1}{1+2\\frac{e}{e_\\mathrm{tot}}} \\) is the neighborhood radius.

    Parameters
    ----------
    dataset : Union[np.ndarray, Callable]
        Training dataset array or generator builder.
    n_epochs : Optional[int]
        Number of epochs to train for (default: **None**).
    n_vectors : Optional[int]
        Number of vectors to train for (default: **None**).
    n_error_bins : int
        Number of quantization and topographic error bins (default: **10**).
    show_progress_bar : bool
        Specifies whether to display a progress bar (default: **False**).
    &#34;&#34;&#34;

    ################################################################################################################

    generator_builder = dataset_to_generator_builder(dataset)

    ################################################################################################################

    cur_vector = 0

    self._n_epochs = n_epochs

    self._n_vectors = n_vectors

    penalty_dist = 2.0 if self._topology == &#39;square&#39; else 1.0

    if not (n_epochs is None) and (n_vectors is None):

        ############################################################################################################
        # TRAINING BY NUMBER OF EPOCHS                                                                             #
        ############################################################################################################

        self._quantization_errors = np.zeros(n_epochs, dtype = np.float32)

        self._topographic_errors = np.zeros(n_epochs, dtype = np.float32)

        ############################################################################################################

        for cur_epoch in tqdm.trange(n_epochs, disable = not show_progress_bar):

            generator = generator_builder()

            for data in generator():

                cur_vector += data.shape[0]

                SOM_Online._train_step1_epoch(
                    self._weights,
                    self._quantization_errors,
                    self._topographic_errors,
                    self._topography,
                    data,
                    cur_epoch,
                    n_epochs,
                    self._alpha,
                    self._sigma,
                    penalty_dist,
                    self._m * self._n
                )

        ############################################################################################################

        if cur_vector &gt; 0:

            self._quantization_errors = self._quantization_errors * n_epochs / cur_vector

            self._topographic_errors = self._topographic_errors * n_epochs / cur_vector

        ############################################################################################################

    elif (n_epochs is None) and not (n_vectors is None):

        ############################################################################################################
        # TRAINING BY NUMBER OF VECTORS                                                                            #
        ############################################################################################################

        self._quantization_errors = np.zeros(n_error_bins, dtype = np.float32)

        self._topographic_errors = np.zeros(n_error_bins, dtype = np.float32)

        ############################################################################################################

        progress_bar = tqdm.tqdm(total = n_vectors, disable = not show_progress_bar)

        generator = generator_builder()

        for data in generator():

            count = min(data.shape[0], n_vectors - cur_vector)

            SOM_Online._train_step1_iter(
                self._weights,
                self._quantization_errors,
                self._topographic_errors,
                self._topography,
                data[0: count],
                cur_vector,
                n_vectors,
                n_error_bins,
                self._alpha,
                self._sigma,
                penalty_dist,
                self._m * self._n
            )

            cur_vector += count

            progress_bar.update(count)

            if cur_vector &gt;= n_vectors:

                break

        ############################################################################################################

        if cur_vector &gt; 0:

            self._quantization_errors = self._quantization_errors * n_error_bins / cur_vector

            self._topographic_errors = self._topographic_errors * n_error_bins / cur_vector

        ############################################################################################################

    else:

        raise Exception(&#39;Invalid training method, specify either `n_epochs` or `n_vectors`.&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="decontamination.Clustering"><code class="flex name class">
<span>class <span class="ident">Clustering</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Clustering(object):

    ####################################################################################################################

    @staticmethod
    def _init_distances(weights: np.ndarray, enable_gpu: bool, threads_per_blocks: int) -&gt; np.ndarray:

        result = device_array_full(2 * (weights.shape[0], ), np.inf, dtype = np.float32)

        _init_distances_kernel[enable_gpu, 2 * (threads_per_blocks, ), 2 * (weights.shape[0], )](
            result,
            weights
        )

        return result.copy_to_host()

    ####################################################################################################################

    @staticmethod
    @nb.njit(parallel = False)
    def _update_clusters(dist: np.ndarray, clusters: np.ndarray) -&gt; None:

        ################################################################################################################

        index = np.argmin(dist)

        j, i = divmod(index, dist.shape[0])

        ################################################################################################################

        dist[i, : i] = np.maximum(dist[i, : i], dist[j, : i])
        dist[j + 1:, i] = np.maximum(dist[j + 1:, i], dist[j + 1:, j])
        dist[i + 1: j, i] = np.maximum(dist[i + 1: j, i], dist[j, i + 1: j])

        ################################################################################################################

        dist[j, : i] = np.inf
        dist[j, i: j] = np.inf
        dist[j + 1:, j] = np.inf

        ################################################################################################################

        clusters[np.where(clusters == clusters[j])[0]] = clusters[i]

    ####################################################################################################################

    @staticmethod
    def clusterize(vectors: np.ndarray, n_clusters: int, enable_gpu: bool = True, threads_per_blocks: int = 32) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Clusterizes the input array using Lance-Williams hierarchical clustering with complete-linkage.

        Parameters
        ----------
        vectors : np.ndarray
            Flat input array to be clustered.
        n_clusters : int
            Desired number of clusters.
        enable_gpu : bool
            If available, run on GPU rather than CPU (default: **True**).
        threads_per_blocks : int
            Number of GPU threads per blocks (default: **32**).

        Note
        ----

        Number of iterations: weights.shape[0] - n_clusters.

        Return
        ------
        Array giving a cluster identifier for each input vector.
        &#34;&#34;&#34;

        ################################################################################################################

        distances = Clustering._init_distances(vectors, enable_gpu = enable_gpu, threads_per_blocks = threads_per_blocks)

        ################################################################################################################

        result = np.arange(vectors.shape[0])

        for _ in range(vectors.shape[0] - n_clusters):

            Clustering._update_clusters(distances, result)

        ################################################################################################################

        return result

    ####################################################################################################################

    @staticmethod
    def average(vectors: np.ndarray, cluster_ids: np.ndarray) -&gt; np.ndarray:

        &#34;&#34;&#34;
        Performs averaging per cluster.

        Parameters
        ----------
        vectors : np.ndarray
            Flat input array be averaged.
        cluster_ids : np.ndarray
            Array of cluster identifiers.
        &#34;&#34;&#34;

        result = np.empty_like(vectors)

        for cluster_id in np.unique(cluster_ids):

            cluster_indices = np.where(cluster_ids == cluster_id)[0]

            result[cluster_indices] = np.nanmean(vectors[cluster_indices], axis = 0)

        return result</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="decontamination.Clustering.clusterize"><code class="name flex">
<span>def <span class="ident">clusterize</span></span>(<span>vectors:Â np.ndarray, n_clusters:Â int, enable_gpu:Â boolÂ =Â True, threads_per_blocks:Â intÂ =Â 32) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Clusterizes the input array using Lance-Williams hierarchical clustering with complete-linkage.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vectors</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Flat input array to be clustered.</dd>
<dt><strong><code>n_clusters</code></strong> :&ensp;<code>int</code></dt>
<dd>Desired number of clusters.</dd>
<dt><strong><code>enable_gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>If available, run on GPU rather than CPU (default: <strong>True</strong>).</dd>
<dt><strong><code>threads_per_blocks</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of GPU threads per blocks (default: <strong>32</strong>).</dd>
</dl>
<h2 id="note">Note</h2>
<p>Number of iterations: weights.shape[0] - n_clusters.</p>
<h2 id="return">Return</h2>
<p>Array giving a cluster identifier for each input vector.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clusterize(vectors: np.ndarray, n_clusters: int, enable_gpu: bool = True, threads_per_blocks: int = 32) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Clusterizes the input array using Lance-Williams hierarchical clustering with complete-linkage.

    Parameters
    ----------
    vectors : np.ndarray
        Flat input array to be clustered.
    n_clusters : int
        Desired number of clusters.
    enable_gpu : bool
        If available, run on GPU rather than CPU (default: **True**).
    threads_per_blocks : int
        Number of GPU threads per blocks (default: **32**).

    Note
    ----

    Number of iterations: weights.shape[0] - n_clusters.

    Return
    ------
    Array giving a cluster identifier for each input vector.
    &#34;&#34;&#34;

    ################################################################################################################

    distances = Clustering._init_distances(vectors, enable_gpu = enable_gpu, threads_per_blocks = threads_per_blocks)

    ################################################################################################################

    result = np.arange(vectors.shape[0])

    for _ in range(vectors.shape[0] - n_clusters):

        Clustering._update_clusters(distances, result)

    ################################################################################################################

    return result</code></pre>
</details>
</dd>
<dt id="decontamination.Clustering.average"><code class="name flex">
<span>def <span class="ident">average</span></span>(<span>vectors:Â np.ndarray, cluster_ids:Â np.ndarray) â€‘>Â np.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Performs averaging per cluster.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vectors</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Flat input array be averaged.</dd>
<dt><strong><code>cluster_ids</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array of cluster identifiers.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def average(vectors: np.ndarray, cluster_ids: np.ndarray) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Performs averaging per cluster.

    Parameters
    ----------
    vectors : np.ndarray
        Flat input array be averaged.
    cluster_ids : np.ndarray
        Array of cluster identifiers.
    &#34;&#34;&#34;

    result = np.empty_like(vectors)

    for cluster_id in np.unique(cluster_ids):

        cluster_indices = np.where(cluster_ids == cluster_id)[0]

        result[cluster_indices] = np.nanmean(vectors[cluster_indices], axis = 0)

    return result</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="decontamination.device_array_from" href="#decontamination.device_array_from">device_array_from</a></code></li>
<li><code><a title="decontamination.device_array_empty" href="#decontamination.device_array_empty">device_array_empty</a></code></li>
<li><code><a title="decontamination.device_array_zeros" href="#decontamination.device_array_zeros">device_array_zeros</a></code></li>
<li><code><a title="decontamination.device_array_full" href="#decontamination.device_array_full">device_array_full</a></code></li>
<li><code><a title="decontamination.display_latent_space" href="#decontamination.display_latent_space">display_latent_space</a></code></li>
<li><code><a title="decontamination.display_clusters" href="#decontamination.display_clusters">display_clusters</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="decontamination.jit" href="#decontamination.jit">jit</a></code></h4>
<ul class="two-column">
<li><code><a title="decontamination.jit.grid" href="#decontamination.jit.grid">grid</a></code></li>
<li><code><a title="decontamination.jit.local_empty" href="#decontamination.jit.local_empty">local_empty</a></code></li>
<li><code><a title="decontamination.jit.shared_empty" href="#decontamination.jit.shared_empty">shared_empty</a></code></li>
<li><code><a title="decontamination.jit.syncthreads" href="#decontamination.jit.syncthreads">syncthreads</a></code></li>
<li><code><a title="decontamination.jit.atomic_add" href="#decontamination.jit.atomic_add">atomic_add</a></code></li>
<li><code><a title="decontamination.jit.atomic_sub" href="#decontamination.jit.atomic_sub">atomic_sub</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="decontamination.DeviceArray" href="#decontamination.DeviceArray">DeviceArray</a></code></h4>
<ul class="">
<li><code><a title="decontamination.DeviceArray.copy_to_host" href="#decontamination.DeviceArray.copy_to_host">copy_to_host</a></code></li>
<li><code><a title="decontamination.DeviceArray.shape" href="#decontamination.DeviceArray.shape">shape</a></code></li>
<li><code><a title="decontamination.DeviceArray.dtype" href="#decontamination.DeviceArray.dtype">dtype</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="decontamination.SOM_Abstract" href="#decontamination.SOM_Abstract">SOM_Abstract</a></code></h4>
<ul class="">
<li><code><a title="decontamination.SOM_Abstract.init_rand" href="#decontamination.SOM_Abstract.init_rand">init_rand</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.init_from" href="#decontamination.SOM_Abstract.init_from">init_from</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.save" href="#decontamination.SOM_Abstract.save">save</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.load" href="#decontamination.SOM_Abstract.load">load</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_weights" href="#decontamination.SOM_Abstract.get_weights">get_weights</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_centroids" href="#decontamination.SOM_Abstract.get_centroids">get_centroids</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_quantization_errors" href="#decontamination.SOM_Abstract.get_quantization_errors">get_quantization_errors</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_topographic_errors" href="#decontamination.SOM_Abstract.get_topographic_errors">get_topographic_errors</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_distance_map" href="#decontamination.SOM_Abstract.get_distance_map">get_distance_map</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_activation_map" href="#decontamination.SOM_Abstract.get_activation_map">get_activation_map</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.get_winners" href="#decontamination.SOM_Abstract.get_winners">get_winners</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.m" href="#decontamination.SOM_Abstract.m">m</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.n" href="#decontamination.SOM_Abstract.n">n</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.dim" href="#decontamination.SOM_Abstract.dim">dim</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.dtype" href="#decontamination.SOM_Abstract.dtype">dtype</a></code></li>
<li><code><a title="decontamination.SOM_Abstract.topology" href="#decontamination.SOM_Abstract.topology">topology</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="decontamination.SOM_PCA" href="#decontamination.SOM_PCA">SOM_PCA</a></code></h4>
<ul class="">
<li><code><a title="decontamination.SOM_PCA.train" href="#decontamination.SOM_PCA.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="decontamination.SOM_Batch" href="#decontamination.SOM_Batch">SOM_Batch</a></code></h4>
<ul class="">
<li><code><a title="decontamination.SOM_Batch.train" href="#decontamination.SOM_Batch.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="decontamination.SOM_Online" href="#decontamination.SOM_Online">SOM_Online</a></code></h4>
<ul class="">
<li><code><a title="decontamination.SOM_Online.train" href="#decontamination.SOM_Online.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="decontamination.Clustering" href="#decontamination.Clustering">Clustering</a></code></h4>
<ul class="">
<li><code><a title="decontamination.Clustering.clusterize" href="#decontamination.Clustering.clusterize">clusterize</a></code></li>
<li><code><a title="decontamination.Clustering.average" href="#decontamination.Clustering.average">average</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>